---
phase: 02-signal-layer-baseline-model
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/hydra/signals/sentiment/__init__.py
  - src/hydra/signals/sentiment/cot_scoring.py
  - tests/test_cot_scoring.py
autonomous: true
requirements:
  - SGNL-01

must_haves:
  truths:
    - "COT managed money net positioning is normalized to [-1, +1] via 52-week percentile rank"
    - "Sentiment confidence weight in [0, 1] derived from OI magnitude and positioning concentration"
    - "Insufficient history (< 4 weeks) returns neutral score (0.0) with zero confidence"
    - "Extreme positioning (> 90th or < 10th percentile) produces scores near +1 or -1"
  artifacts:
    - path: "src/hydra/signals/sentiment/cot_scoring.py"
      provides: "SentimentScore dataclass and compute_cot_sentiment function"
      exports: ["SentimentScore", "compute_cot_sentiment"]
    - path: "src/hydra/signals/sentiment/__init__.py"
      provides: "Public re-exports for sentiment module"
    - path: "tests/test_cot_scoring.py"
      provides: "TDD tests for COT sentiment scoring"
      min_lines: 60
  key_links:
    - from: "src/hydra/signals/sentiment/cot_scoring.py"
      to: "scipy.stats.percentileofscore"
      via: "percentile rank computation"
      pattern: "percentileofscore"
    - from: "src/hydra/signals/sentiment/cot_scoring.py"
      to: "src/hydra/data/store/feature_store.py"
      via: "reads cot_managed_money_net, cot_total_oi feature history"
      pattern: "get_feature_history|get_features_at"
---

<objective>
Build the COT sentiment scoring module that transforms raw CFTC managed money positioning into a normalized sentiment score in [-1, +1] with a confidence weight in [0, 1].

Purpose: This is the first half of the divergence signal -- sentiment reflects crowd positioning. The divergence detector (Plan 02-03) compares this against options-implied expectations.

Output: `src/hydra/signals/sentiment/cot_scoring.py` with `SentimentScore` dataclass and `compute_cot_sentiment()` function, fully tested via TDD.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/hydra/data/ingestion/cot.py
@src/hydra/data/store/feature_store.py
@.planning/phases/02-signal-layer-baseline-model/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for COT sentiment scoring</name>
  <files>tests/test_cot_scoring.py, src/hydra/signals/sentiment/__init__.py</files>
  <action>
Create `src/hydra/signals/sentiment/__init__.py` (empty for now).

Create `tests/test_cot_scoring.py` with these test cases:

1. **test_neutral_on_insufficient_history**: Pass < 4 weeks of history -> score=0.0, confidence=0.0
2. **test_extreme_bullish**: managed_money_net at the maximum of a 52-week history -> score near +1.0
3. **test_extreme_bearish**: managed_money_net at the minimum of a 52-week history -> score near -1.0
4. **test_median_positioning**: managed_money_net at the median of history -> score near 0.0
5. **test_confidence_scales_with_oi**: Higher total_oi relative to history produces higher confidence
6. **test_confidence_scales_with_concentration**: Higher abs(managed_money_net)/total_oi increases confidence
7. **test_score_clamped_to_bounds**: Score always in [-1, +1], confidence always in [0, 1]
8. **test_components_populated**: components dict contains managed_money_pct_rank, oi_rank, concentration

Use numpy to generate synthetic 52-week history arrays. The SentimentScore dataclass must have fields: score (float), confidence (float), components (dict).

Import from `hydra.signals.sentiment.cot_scoring import SentimentScore, compute_cot_sentiment`.

Run tests -- they MUST fail (module not yet implemented).
  </action>
  <verify>`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/test_cot_scoring.py -v` shows ImportError or failures (RED phase)</verify>
  <done>Test file exists with 8+ test cases, all failing because implementation does not exist yet</done>
</task>

<task type="auto">
  <name>Task 2: GREEN + REFACTOR -- Implement COT sentiment scoring</name>
  <files>src/hydra/signals/sentiment/cot_scoring.py, src/hydra/signals/sentiment/__init__.py</files>
  <action>
Implement `src/hydra/signals/sentiment/cot_scoring.py` following research code example exactly:

```python
@dataclass
class SentimentScore:
    score: float       # [-1, +1]: -1 bearish, +1 bullish
    confidence: float  # [0, 1]: confidence in the score
    components: dict   # breakdown: managed_money_pct_rank, oi_rank, concentration
```

`compute_cot_sentiment(managed_money_net, producer_net, total_oi, history_managed, history_oi) -> SentimentScore`:

1. If len(history_managed) < 4, return SentimentScore(0.0, 0.0, {})
2. pct_rank = percentileofscore(history_managed, managed_money_net) / 100.0
3. score = 2.0 * pct_rank - 1.0, clipped to [-1, +1]
4. Confidence = 0.6 * oi_rank + 0.4 * min(concentration * 5.0, 1.0), clipped to [0, 1]
   - oi_rank = percentileofscore(history_oi, total_oi) / 100.0
   - concentration = abs(managed_money_net) / max(total_oi, 1)
5. Return SentimentScore with components dict

Dependencies: numpy, scipy.stats.percentileofscore (already in project).

Update `__init__.py` to re-export: `from hydra.signals.sentiment.cot_scoring import SentimentScore, compute_cot_sentiment`.

Run all tests -- they MUST pass (GREEN phase). Then review for clean code (REFACTOR).
  </action>
  <verify>`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/test_cot_scoring.py -v` -- all tests pass</verify>
  <done>All 8+ tests pass. SentimentScore(score=float, confidence=float, components=dict) returned correctly for all edge cases.</done>
</task>

</tasks>

<verification>
```bash
cd /Users/tristanfarmer/Documents/HYDRA
python -m pytest tests/test_cot_scoring.py -v
python -c "from hydra.signals.sentiment import SentimentScore, compute_cot_sentiment; print('Import OK')"
```
</verification>

<success_criteria>
- compute_cot_sentiment returns score in [-1, +1] for all inputs
- Confidence in [0, 1] for all inputs
- Insufficient history returns neutral (0.0, 0.0)
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-signal-layer-baseline-model/02-01-SUMMARY.md`
</output>
