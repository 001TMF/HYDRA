---
phase: 03-sandbox-experiment-infrastructure
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/hydra/sandbox/evaluator.py
  - tests/test_evaluator.py
autonomous: true
requirements: [SBOX-05]

must_haves:
  truths:
    - "Composite fitness score combines 6 metrics with specified weights: Sharpe 0.25, drawdown 0.20, calibration 0.15, robustness 0.15, slippage-adjusted return 0.15, simplicity 0.10"
    - "Each metric is min-max normalized to [0, 1] before weighting to prevent scale domination"
    - "Calibration uses Brier score (lower is better -- inverted in normalization)"
    - "Robustness is the fraction of walk-forward folds with Sharpe > 0"
    - "Simplicity penalizes model complexity via 1/log2(n_features * n_estimators + 1)"
    - "Final fitness score is in [0, 1] range"
  artifacts:
    - path: "src/hydra/sandbox/evaluator.py"
      provides: "CompositeEvaluator with 6-metric weighted fitness scoring"
      exports: ["CompositeEvaluator", "FitnessScore"]
    - path: "tests/test_evaluator.py"
      provides: "Tests for normalization, weighting, edge cases, metric definitions"
  key_links:
    - from: "src/hydra/sandbox/evaluator.py"
      to: "src/hydra/model/evaluation.py"
      via: "Uses BacktestResult metrics (sharpe_ratio, max_drawdown, hit_rate, fold_sharpes)"
      pattern: "BacktestResult"
---

<objective>
Create the composite fitness evaluator that scores candidate models on 6 weighted metrics with min-max normalization.

Purpose: SBOX-05 defines how candidate models are ranked against each other and the champion. The composite fitness score is the single number that determines promotion decisions. Getting normalization right prevents scale domination (Sharpe [-2,3] vs hit_rate [0,1]).

Output: `src/hydra/sandbox/evaluator.py` with CompositeEvaluator and full test suite.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-sandbox-experiment-infrastructure/03-RESEARCH.md
@src/hydra/model/evaluation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CompositeEvaluator with normalized weighted scoring</name>
  <files>src/hydra/sandbox/evaluator.py</files>
  <action>
Create `src/hydra/sandbox/evaluator.py` with:

1. **Module-level constants**:
   ```python
   DEFAULT_WEIGHTS = {
       "sharpe": 0.25,
       "drawdown": 0.20,
       "calibration": 0.15,
       "robustness": 0.15,
       "slippage_adjusted_return": 0.15,
       "simplicity": 0.10,
   }

   DEFAULT_RANGES = {
       "sharpe": (-1.0, 3.0),
       "drawdown": (-0.50, 0.0),
       "calibration": (0.0, 0.5),           # Brier score: 0.5 is random, 0 is perfect
       "robustness": (0.0, 1.0),
       "slippage_adjusted_return": (-0.50, 1.0),
       "simplicity": (0.0, 1.0),
   }
   ```

2. **FitnessScore dataclass**:
   - `composite: float` (weighted sum in [0, 1])
   - `components: dict[str, float]` (metric_name -> normalized value in [0, 1])
   - `raw_metrics: dict[str, float]` (metric_name -> raw value before normalization)
   - `weights: dict[str, float]` (the weights used)

3. **CompositeEvaluator class**:
   - `__init__(self, weights: dict[str, float] | None = None, ranges: dict[str, float] | None = None)`:
     - `self.weights = weights or DEFAULT_WEIGHTS`.
     - `self.ranges = ranges or DEFAULT_RANGES`.
     - Validate weights sum to 1.0 (within tolerance 1e-6). Raise ValueError if not.

   - `score(self, metrics: dict[str, float]) -> FitnessScore`:
     - For each metric in weights:
       - Get raw value from `metrics` dict. Raise KeyError if missing.
       - Normalize to [0, 1] using `np.clip((raw - lo) / (hi - lo), 0, 1)`.
       - **Special handling for calibration (Brier score)**: Lower is better, so invert: `normalized = 1.0 - np.clip((raw - lo) / (hi - lo), 0, 1)`.
       - **Special handling for drawdown**: Already negative, range is (-0.50, 0.0). Higher (closer to 0) is better. Formula naturally handles this since 0.0 maps to 1.0 and -0.50 maps to 0.0.
     - Weighted sum: `composite = sum(weight * normalized for each metric)`.
     - Return FitnessScore with composite, components (normalized values), raw_metrics, weights.

   - `compute_robustness(self, fold_sharpes: list[float]) -> float`:
     - Static method. Returns fraction of folds where fold Sharpe > 0.
     - `len([s for s in fold_sharpes if s > 0]) / len(fold_sharpes)` if non-empty, else 0.0.

   - `compute_simplicity(self, n_features: int, n_estimators: int) -> float`:
     - Static method. Returns `1.0 / np.log2(n_features * n_estimators + 1)`.
     - Clipped to [0, 1].

   - `compute_calibration(self, probabilities: np.ndarray, actuals: np.ndarray) -> float`:
     - Static method. Returns Brier score: `float(np.mean((probabilities - actuals) ** 2))`.
     - Lower is better.

   - `score_from_backtest(self, backtest_result, probabilities: np.ndarray, actuals: np.ndarray, n_features: int, n_estimators: int) -> FitnessScore`:
     - Convenience method that extracts metrics from a BacktestResult-like object.
     - Computes robustness from `backtest_result.fold_sharpes`.
     - Computes simplicity from n_features and n_estimators.
     - Computes calibration from probabilities and actuals.
     - Assembles the metrics dict and calls `self.score()`.
     - Uses duck-typing on backtest_result (access .sharpe_ratio, .max_drawdown, .total_return, .fold_sharpes) -- no direct import of BacktestResult to avoid coupling.
  </action>
  <verify>
`python -c "from hydra.sandbox.evaluator import CompositeEvaluator, FitnessScore; print('OK')"` succeeds.
  </verify>
  <done>CompositeEvaluator with 6-metric weighted scoring, min-max normalization, inverted calibration (Brier score), robustness/simplicity/calibration helper methods.</done>
</task>

<task type="auto">
  <name>Task 2: Test composite evaluator scoring and normalization</name>
  <files>tests/test_evaluator.py</files>
  <action>
Create `tests/test_evaluator.py` with:

1. **test_weights_sum_validation**: Attempt to create evaluator with weights summing to 0.5. Assert ValueError.

2. **test_perfect_score**: Pass metrics at the top of each range (sharpe=3.0, drawdown=0.0, calibration=0.0, robustness=1.0, slippage_adjusted_return=1.0, simplicity=1.0). Assert composite close to 1.0.

3. **test_worst_score**: Pass metrics at the bottom of each range. Assert composite close to 0.0.

4. **test_mid_range_score**: Pass metrics at midpoints. Assert composite close to 0.5.

5. **test_calibration_inverted**: Higher Brier score (worse calibration) should produce lower normalized value. Score with calibration=0.4 vs calibration=0.1. Assert the 0.1 score has higher calibration component.

6. **test_clipping**: Pass sharpe=10.0 (above range max 3.0). Assert normalized component is 1.0 (clipped, not >1).

7. **test_compute_robustness**: fold_sharpes=[0.5, -0.1, 0.3, 0.8, -0.2]. Assert robustness = 3/5 = 0.6.

8. **test_compute_simplicity**: n_features=17, n_estimators=100. Assert simplicity is a float in (0, 1).

9. **test_compute_calibration**: Perfect calibration (probabilities == actuals as float). Assert Brier score close to 0.

10. **test_score_from_backtest**: Create a mock backtest result object with sharpe_ratio, max_drawdown, total_return, fold_sharpes. Call score_from_backtest. Assert returns valid FitnessScore with all 6 components.

11. **test_components_in_range**: Score with arbitrary valid metrics. Assert every component value in [0, 1]. Assert composite in [0, 1].
  </action>
  <verify>`python -m pytest tests/test_evaluator.py -v` -- all tests pass.</verify>
  <done>11 tests covering weight validation, perfect/worst/mid scores, calibration inversion, clipping, robustness, simplicity, calibration computation, backtest integration, and range bounds. All pass.</done>
</task>

</tasks>

<verification>
```bash
python -m pytest tests/test_evaluator.py -v
python -c "from hydra.sandbox.evaluator import CompositeEvaluator, FitnessScore; print('OK')"
```
All tests pass. Evaluator produces normalized composite fitness in [0, 1].
</verification>

<success_criteria>
1. Weights match SBOX-05 specification: Sharpe 0.25, drawdown 0.20, calibration 0.15, robustness 0.15, slippage-adjusted return 0.15, simplicity 0.10
2. Min-max normalization prevents scale domination
3. Calibration (Brier score) is correctly inverted (lower raw = higher normalized)
4. Robustness = fraction of folds with Sharpe > 0
5. Simplicity = 1/log2(n_features * n_estimators + 1)
6. Composite score always in [0, 1]
7. All 11 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-sandbox-experiment-infrastructure/03-05-SUMMARY.md`
</output>
