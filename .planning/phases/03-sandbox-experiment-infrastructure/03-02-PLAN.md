---
phase: 03-sandbox-experiment-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/hydra/sandbox/registry.py
  - tests/test_registry.py
  - pyproject.toml
autonomous: true
requirements: [SBOX-02]

must_haves:
  truths:
    - "Models are logged to MLflow with full config snapshot, all metrics, and custom tags"
    - "Champion/candidate/archived lifecycle is enforced via MLflow aliases (not deprecated stages)"
    - "Promoting a candidate to champion automatically archives the current champion"
    - "Rollback swaps champion alias to the previously archived version"
    - "Loading champion model returns a usable LightGBM model that can predict"
  artifacts:
    - path: "src/hydra/sandbox/registry.py"
      provides: "ModelRegistry wrapping MLflow with lifecycle conventions"
      exports: ["ModelRegistry"]
    - path: "tests/test_registry.py"
      provides: "Tests for model logging, promotion, rollback, loading"
    - path: "pyproject.toml"
      provides: "mlflow dependency added"
      contains: "mlflow"
  key_links:
    - from: "src/hydra/sandbox/registry.py"
      to: "mlflow"
      via: "mlflow.lightgbm.log_model and MlflowClient aliases API"
      pattern: "mlflow\\.lightgbm"
    - from: "src/hydra/sandbox/registry.py"
      to: "src/hydra/model/baseline.py"
      via: "BaselineModel.model attribute (underlying LGBMClassifier) for logging"
      pattern: "model\\.model"
---

<objective>
Create the MLflow model registry wrapper that enforces HYDRA's champion/candidate/archived lifecycle using MLflow aliases.

Purpose: SBOX-02 requires every trained model to be tracked with full config snapshot, metrics, and lifecycle state. The registry wrapper provides a thin, convention-enforcing layer over MLflow's Python client so the experiment loop (Phase 4) has a clean API for model management.

Output: `src/hydra/sandbox/registry.py` with ModelRegistry class and full test suite.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-sandbox-experiment-infrastructure/03-RESEARCH.md
@src/hydra/model/baseline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add MLflow dependency and create ModelRegistry</name>
  <files>pyproject.toml, src/hydra/sandbox/registry.py</files>
  <action>
Add `"mlflow>=3.9"` to `dependencies` in `pyproject.toml`. Run `uv sync` to install.

Create `src/hydra/sandbox/registry.py` with:

1. **ModelRegistry class**:
   - `__init__(self, tracking_uri: str | None = None, model_name: str = "hydra-baseline")`:
     - If `tracking_uri` is None, default to `file://{project_root}/mlruns` using absolute path (prevents the relative-path pitfall from research).
     - Call `mlflow.set_tracking_uri(tracking_uri)`.
     - Create `self.client = mlflow.MlflowClient()`.
     - Store `self.model_name = model_name`.
     - Ensure the registered model exists: try `client.get_registered_model(model_name)`, if `MlflowException` then `client.create_registered_model(model_name)`.

   - `log_candidate(self, model, metrics: dict, config: dict, tags: dict | None = None, run_name: str | None = None) -> tuple[str, int]`:
     - Start an MLflow run with optional `run_name`.
     - `mlflow.log_params(config)` for full config snapshot.
     - `mlflow.log_metrics(metrics)` for all evaluation metrics.
     - `mlflow.lightgbm.log_model(model.model, artifact_path="model", registered_model_name=self.model_name)` -- accesses the underlying LGBMClassifier via `model.model`.
     - Set tags: `mlflow.set_tag(k, v)` for each tag.
     - Return `(run_id, model_version_number)` -- get version from the registered model versions list (latest version).

   - `promote_to_champion(self, version: int) -> None`:
     - Try to get current champion via `client.get_model_version_by_alias(self.model_name, "champion")`.
     - If champion exists, archive it: `client.set_registered_model_alias(self.model_name, "archived", int(current.version))`.
     - Set new champion: `client.set_registered_model_alias(self.model_name, "champion", version)`.

   - `rollback(self) -> int`:
     - Get archived version via `client.get_model_version_by_alias(self.model_name, "archived")`.
     - Call `promote_to_champion(int(archived.version))`.
     - Return the new champion version number.
     - Raise `ValueError("No archived model to rollback to")` if no archived alias exists.

   - `load_champion(self)`:
     - Load via `mlflow.lightgbm.load_model(f"models:/{self.model_name}@champion")`.
     - Return the loaded model (a LightGBM Booster or sklearn-compatible model).
     - Raise `ValueError("No champion model set")` if alias doesn't exist.

   - `get_champion_info(self) -> dict`:
     - Get champion version via alias.
     - Return dict with keys: `version`, `run_id`, `metrics` (from run), `tags`, `created_at`.

   - `list_versions(self) -> list[dict]`:
     - List all model versions for `self.model_name`.
     - Return list of dicts with: `version`, `run_id`, `aliases`, `created_at`.

Use explicit logging (NOT autolog) per research recommendation. Import `mlflow`, `mlflow.lightgbm`, and `from mlflow import MlflowClient`. Handle `MlflowException` from `mlflow.exceptions`.
  </action>
  <verify>
`python -c "from hydra.sandbox.registry import ModelRegistry; print('OK')"` succeeds.
  </verify>
  <done>ModelRegistry class exists with log_candidate, promote_to_champion, rollback, load_champion, get_champion_info, list_versions. Uses MLflow aliases (not deprecated stages). Absolute tracking URI default prevents path confusion.</done>
</task>

<task type="auto">
  <name>Task 2: Test ModelRegistry with real MLflow operations</name>
  <files>tests/test_registry.py</files>
  <action>
Create `tests/test_registry.py` with tests using a temporary directory for MLflow tracking:

1. **test_log_candidate**: Create a BaselineModel, train on small synthetic data. Log via `registry.log_candidate()`. Assert returns (run_id, version) where run_id is a string and version is an int >= 1.

2. **test_promote_and_load_champion**: Log a candidate, promote to champion. Call `load_champion()`. Assert the loaded model can predict (call `predict` or `predict_proba` on dummy data -- MLflow loads as a LightGBM Booster, so use `booster.predict(data)` pattern).

3. **test_promote_archives_previous**: Log candidate A, promote A. Log candidate B, promote B. Assert A's version now has "archived" alias. Verify via `get_champion_info()` that champion is B.

4. **test_rollback**: Log A, promote A. Log B, promote B. Rollback. Assert champion is now A again (via `get_champion_info()`).

5. **test_rollback_no_archived_raises**: Fresh registry with no models. Assert `ValueError` raised on rollback.

6. **test_list_versions**: Log two candidates. `list_versions()` returns list of length 2 with correct fields.

Use `tmp_path` pytest fixture for MLflow tracking URI: `f"file://{tmp_path}/mlruns"`. Each test gets a fresh registry pointed at its own temp dir.

Create a helper fixture:
```python
@pytest.fixture
def registry(tmp_path):
    return ModelRegistry(tracking_uri=f"file://{tmp_path}/mlruns")

@pytest.fixture
def trained_model():
    X = np.random.RandomState(42).randn(50, 5)
    y = (X[:, 0] > 0).astype(int)
    model = BaselineModel()
    model.train(X, y, [f"f{i}" for i in range(5)])
    return model
```
  </action>
  <verify>`python -m pytest tests/test_registry.py -v` -- all tests pass.</verify>
  <done>6 tests covering logging, promotion, archival, rollback, error handling, and version listing. All use real MLflow operations against temp directories.</done>
</task>

</tasks>

<verification>
```bash
python -m pytest tests/test_registry.py -v
python -c "from hydra.sandbox.registry import ModelRegistry; print('OK')"
```
All tests pass. Registry wraps MLflow with alias-based lifecycle.
</verification>

<success_criteria>
1. ModelRegistry logs models with full config, metrics, and tags via explicit MLflow logging
2. Champion/candidate/archived lifecycle uses MLflow aliases (not deprecated stages)
3. Promotion automatically archives current champion
4. Rollback restores previously archived version
5. Absolute tracking URI default prevents directory confusion
6. All 6 tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-sandbox-experiment-infrastructure/03-02-SUMMARY.md`
</output>
