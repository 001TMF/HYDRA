---
phase: 01-data-infrastructure-options-math-engine
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - src/hydra/data/ingestion/base.py
  - src/hydra/data/ingestion/futures.py
  - src/hydra/data/ingestion/options.py
  - src/hydra/data/ingestion/cot.py
  - tests/test_ingestion_futures.py
  - tests/test_ingestion_options.py
  - tests/test_ingestion_cot.py
autonomous: true
requirements:
  - DATA-01
  - DATA-02
  - DATA-03

user_setup:
  - service: databento
    why: "Primary data vendor for CME futures and options chain data"
    env_vars:
      - name: DATABENTO_API_KEY
        source: "Sign up at https://databento.com (free $125 credits), then Settings -> API Keys"
    dashboard_config: []

must_haves:
  truths:
    - "Futures OHLCV pipeline fetches daily bars from Databento and persists to Parquet lake"
    - "Options chain pipeline fetches bid/ask/OI/strike/expiry from Databento and persists to Parquet"
    - "COT pipeline fetches disaggregated futures-only reports and writes with correct Tuesday as_of / Friday available_at"
    - "All three pipelines follow the abstract IngestPipeline interface: fetch -> validate -> persist"
    - "COT features are written to the feature store with Friday available_at timestamps"
  artifacts:
    - path: "src/hydra/data/ingestion/base.py"
      provides: "Abstract IngestPipeline base class"
      exports: ["IngestPipeline"]
      min_lines: 30
    - path: "src/hydra/data/ingestion/futures.py"
      provides: "Databento futures OHLCV ingestion"
      exports: ["FuturesIngestPipeline"]
      min_lines: 60
    - path: "src/hydra/data/ingestion/options.py"
      provides: "Databento options chain ingestion"
      exports: ["OptionsIngestPipeline"]
      min_lines: 80
    - path: "src/hydra/data/ingestion/cot.py"
      provides: "CFTC COT ingestion with as_of/available_at handling"
      exports: ["COTIngestPipeline"]
      min_lines: 70
  key_links:
    - from: "src/hydra/data/ingestion/futures.py"
      to: "src/hydra/data/store/parquet_lake.py"
      via: "ParquetLake.write() in persist() method"
      pattern: "parquet_lake\\.write"
    - from: "src/hydra/data/ingestion/options.py"
      to: "src/hydra/data/store/parquet_lake.py"
      via: "ParquetLake.write() in persist() method"
      pattern: "parquet_lake\\.write"
    - from: "src/hydra/data/ingestion/cot.py"
      to: "src/hydra/data/store/feature_store.py"
      via: "FeatureStore.write_feature() with Friday available_at"
      pattern: "feature_store\\.write_feature"
---

<objective>
Build the three data ingestion pipelines (futures OHLCV, options chain, CFTC COT) that fetch raw market data and persist it to the Parquet lake and feature store.

Purpose: Without data flowing in, nothing downstream works. These pipelines are the system's connection to the outside world. The COT pipeline's correct as_of/available_at handling is critical for preventing lookahead bias that would invalidate all backtest results.

Output: Three working ingestion pipelines following a shared abstract interface. Futures and options data flows into Parquet. COT features flow into the feature store with correct timing semantics.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-RESEARCH.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create abstract IngestPipeline base and Databento futures pipeline</name>
  <files>
    src/hydra/data/ingestion/base.py
    src/hydra/data/ingestion/futures.py
    tests/test_ingestion_futures.py
  </files>
  <action>
    **base.py** (Pattern 1 from research):
    - Abstract base class IngestPipeline with:
      - Abstract methods: fetch(market: str, date: datetime) -> dict, validate(raw_data: dict) -> tuple[dict, list[str]], persist(data: dict, market: str, date: datetime) -> None
      - Concrete run(market: str, date: datetime) -> bool that orchestrates fetch -> validate -> persist with structlog logging and error handling
      - Constructor accepts ParquetLake and FeatureStore instances (dependency injection, not creation)
    - Use structlog for all logging with bound context (pipeline name, market, date)

    **futures.py** (Databento futures ingestion, DATA-01):
    - FuturesIngestPipeline(IngestPipeline) with:
      - __init__(api_key: str, parquet_lake: ParquetLake, feature_store: FeatureStore)
      - fetch(): Uses databento.Historical client to call timeseries.get_range() with:
        - dataset="GLBX.MDP3"
        - symbols=[f"{market}.FUT"] (parent symbology, e.g., "HE.FUT")
        - stype_in="parent"
        - schema="ohlcv-1d" for daily bars
        - Date range from the date parameter
      - validate(): Check for: non-empty records, valid OHLCV values (open/high/low/close > 0, volume >= 0), high >= low, close between low and high
      - persist(): Convert to pyarrow Table and write to ParquetLake with data_type="futures"
      - Also write close price as a feature to FeatureStore with as_of = date, available_at = date (futures prices are available same day after close)

    **tests/test_ingestion_futures.py:**
    - Mock the databento client (do NOT make real API calls in tests)
    - test_fetch_returns_raw_data: Mock databento response, verify fetch returns dict with records
    - test_validate_catches_invalid_ohlcv: Pass in data with negative prices, verify warnings
    - test_persist_writes_to_parquet_lake: Verify ParquetLake.write is called with correct data_type="futures"
    - test_run_orchestrates_full_pipeline: Mock all steps, verify run() calls fetch -> validate -> persist in order

    IMPORTANT: All timestamps must be timezone-aware UTC. Use datetime.timezone.utc everywhere, never naive datetimes.
  </action>
  <verify>
    - `uv run pytest tests/test_ingestion_futures.py -v` -- all tests pass
    - FuturesIngestPipeline is importable: `uv run python -c "from hydra.data.ingestion.futures import FuturesIngestPipeline"`
  </verify>
  <done>
    Abstract IngestPipeline provides the fetch/validate/persist contract. FuturesIngestPipeline fetches Databento futures data, validates OHLCV, writes to Parquet lake and feature store. All tests pass with mocked Databento client.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement options chain and COT ingestion pipelines</name>
  <files>
    src/hydra/data/ingestion/options.py
    src/hydra/data/ingestion/cot.py
    tests/test_ingestion_options.py
    tests/test_ingestion_cot.py
  </files>
  <action>
    **options.py** (Databento options chain ingestion, DATA-02):
    - OptionsIngestPipeline(IngestPipeline) with:
      - fetch(): Two Databento calls per the research code examples:
        1. mbp-1 schema for bid/ask: symbols=[f"{market}.OPT"], stype_in="parent"
        2. definition schema for strike/expiry metadata: same symbol
        3. statistics schema for OI data: same symbol
      - Join the three datasets on instrument_id to produce a complete chain: strike, expiry, bid, ask, bid_size, ask_size, oi, volume, is_call
      - validate(): Check for: non-empty chain, strikes > 0, bids >= 0, asks > bids (or warn), expiry dates in the future relative to the data date
      - persist(): Write full chain to ParquetLake with data_type="options"
      - Write per-expiry summary features to FeatureStore: put_call_oi_ratio, total_oi, atm_iv (nearest-to-spot strike implied vol), liquid_strike_count

    **cot.py** (CFTC COT ingestion, DATA-03):
    - COTIngestPipeline(IngestPipeline) with:
      - fetch(): Uses cot_reports library: cot.cot_year(year, cot_report_type='disaggregated_fut')
        - Filter to target market by CFTC contract code (from config, e.g., "054642" for lean hogs)
        - Re-download last 4 weeks to catch CFTC revisions (per research pitfall #2)
      - validate(): Check for: non-empty results, positions sum correctly (long + short + spreading = total), no negative position values
      - persist(): Write to Parquet with data_type="cot" AND to FeatureStore with:
        - CRITICAL: as_of = report Tuesday date, available_at = _next_friday(report_date)
        - Helper function _next_friday(date) that computes the next Friday at 15:30 ET (converted to UTC)
        - Features to write: cot_managed_money_net, cot_producer_net, cot_swap_net, cot_total_oi
        - Each feature normalized and raw

    **tests/test_ingestion_options.py:**
    - Mock databento client
    - test_fetch_joins_mbp_definition_statistics: Verify fetch produces complete chain with all fields
    - test_validate_warns_on_bad_spreads: bid > ask should produce warning
    - test_persist_writes_to_parquet: Verify data_type="options"
    - test_summary_features_written_to_feature_store: Verify at least put_call_oi_ratio and liquid_strike_count are written

    **tests/test_ingestion_cot.py:**
    - Mock cot_reports library
    - test_fetch_filters_by_cftc_code: Verify only target market rows are returned
    - test_as_of_available_at_timing: Verify as_of = Tuesday, available_at = next Friday
    - test_next_friday_calculation: Tuesday Feb 18 2026 -> Friday Feb 20 2026 at 20:30 UTC (15:30 ET)
    - test_features_written_with_correct_timing: Write COT, query feature store on Thursday -> not returned, query on Saturday -> returned
    - test_revision_redownload: Verify last 4 weeks are re-fetched

    IMPORTANT: The COT timing test is the most critical test in Phase 1. If this fails, every backtest built on top will have lookahead bias.
  </action>
  <verify>
    - `uv run pytest tests/test_ingestion_options.py tests/test_ingestion_cot.py -v` -- all tests pass
    - Specifically verify the COT timing test passes: Tuesday data not available until Friday
  </verify>
  <done>
    Options chain pipeline fetches complete chain (bid/ask/OI/strike/expiry) from Databento, validates, persists to Parquet with summary features in the feature store. COT pipeline fetches from cot_reports with correct Tuesday as_of / Friday available_at timing, writes features to the feature store, and re-downloads last 4 weeks for revision handling. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_ingestion_*.py -v` -- all tests pass
2. Abstract base class enforces fetch/validate/persist contract
3. COT timing test: Tuesday as_of, Friday available_at, Wednesday query returns nothing
4. All three pipelines write to Parquet lake via the same ParquetLake interface
5. All timestamps are UTC-aware
</verification>

<success_criteria>
- Three ingestion pipelines (futures, options, COT) all implement IngestPipeline interface
- Futures and options data persisted to Parquet via ParquetLake
- COT features written to FeatureStore with correct timing semantics
- All tests pass with mocked external dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-infrastructure-options-math-engine/01-02-SUMMARY.md`
</output>
