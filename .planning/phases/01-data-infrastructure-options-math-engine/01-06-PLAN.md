---
phase: 01-data-infrastructure-options-math-engine
plan: 06
type: execute
wave: 4
depends_on:
  - 01-02
  - 01-04
  - 01-05
files_modified:
  - src/hydra/data/quality.py
  - tests/test_data_quality.py
autonomous: false
requirements:
  - DATA-06

must_haves:
  truths:
    - "Staleness detection flags futures data older than 1 trading day"
    - "Staleness detection flags options data older than 1 trading day"
    - "Staleness detection flags COT data older than 1 week"
    - "Missing strikes validator detects when liquid strike count drops below threshold"
    - "Call price monotonicity check detects arbitrage violations"
    - "Data quality report summarizes all sources with freshness, completeness, and warnings"
  artifacts:
    - path: "src/hydra/data/quality.py"
      provides: "Data quality monitoring with staleness, validators, and reporting"
      exports: ["DataQualityMonitor", "QualityReport"]
      min_lines: 100
    - path: "tests/test_data_quality.py"
      provides: "Data quality monitoring tests"
      min_lines: 80
  key_links:
    - from: "src/hydra/data/quality.py"
      to: "src/hydra/data/store/parquet_lake.py"
      via: "Reads latest data timestamps from Parquet lake"
      pattern: "parquet_lake\\.read"
    - from: "src/hydra/data/quality.py"
      to: "src/hydra/data/store/feature_store.py"
      via: "Reads latest feature timestamps for staleness"
      pattern: "feature_store\\.get_features_at"
---

<objective>
Implement data quality monitoring that detects staleness, missing strikes, anomalous values, and produces a quality report across all data sources. Includes a human verification checkpoint to visually confirm the full pipeline works end-to-end.

Purpose: Data pipeline silent failures (research pitfall #3) are the most insidious bug in a trading system. Stale data looks exactly like fresh data unless you actively check timestamps. This monitor is the early warning system that prevents the system from trading on garbage.

Output: DataQualityMonitor that checks all data sources for freshness, completeness, and validity, with configurable thresholds and a structured quality report.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-RESEARCH.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-01-SUMMARY.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-02-SUMMARY.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-04-SUMMARY.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-05-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement DataQualityMonitor with staleness, validators, and reporting</name>
  <files>
    src/hydra/data/quality.py
    tests/test_data_quality.py
  </files>
  <action>
    **quality.py:**

    DataQualityMonitor class:
    - __init__(parquet_lake: ParquetLake, feature_store: FeatureStore, config: dict)
      - config contains staleness thresholds from default.yaml
    - check_staleness(market: str, current_time: datetime) -> list[StalenessAlert]
      - For each data source (futures, options, cot):
        - Read latest data timestamp from Parquet lake or feature store
        - Compare to current_time minus threshold (futures: 1 trading day, options: 1 trading day, cot: 7 calendar days)
        - Be weekend/holiday-aware: if current_time is Monday, don't alert for missing Saturday data
        - StalenessAlert: source, last_update, threshold, is_stale, days_since_update
    - check_options_quality(market: str, date: datetime) -> OptionsQualityReport
      - Read latest options chain from Parquet
      - Count liquid strikes (OI >= min_oi AND spread <= max_spread_pct)
      - Check call price monotonicity (calls should decrease with strike for same expiry)
      - Check put-call parity (rough check: C - P ~ F*exp(-rT) - K*exp(-rT) within tolerance)
      - OptionsQualityReport: liquid_strike_count, total_strikes, has_arbitrage_violation, put_call_parity_max_error
    - check_cot_freshness(market: str, current_time: datetime) -> COTFreshnessReport
      - Check that the latest COT data is within 7 calendar days
      - Report the as_of date, available_at date, and days since last update
    - generate_report(market: str, current_time: datetime) -> QualityReport
      - Runs all checks, produces structured report
      - QualityReport: market, timestamp, staleness_alerts, options_quality, cot_freshness, overall_status ("healthy" / "degraded" / "stale")
      - Log report via structlog

    Helper function: is_trading_day(date) -> bool
      - Returns False for weekends. Does NOT attempt to track all exchange holidays (too complex, use weekend-only heuristic for Phase 1).

    **tests/test_data_quality.py:**
    - test_staleness_detects_stale_futures: Write futures data 3 days ago, check on a weekday -> stale
    - test_staleness_ignores_weekends: Write futures data Friday, check on Monday -> NOT stale
    - test_options_quality_counts_liquid_strikes: Create chain with mixed quality, verify count
    - test_call_price_monotonicity: Create monotonically decreasing calls (pass), then non-monotonic (fail)
    - test_cot_freshness_within_threshold: COT data from 5 days ago -> fresh. 10 days ago -> stale.
    - test_generate_report_overall_status: All healthy -> "healthy". Any stale -> "stale".
    - test_weekend_aware_staleness: Friday data checked on Saturday/Sunday -> not stale

    Use configurable thresholds from config dict, not hardcoded values.
  </action>
  <verify>
    - `uv run pytest tests/test_data_quality.py -v` -- all tests pass
    - `uv run pytest tests/ -v` -- full test suite still passes (no regressions)
  </verify>
  <done>
    DataQualityMonitor checks all data sources for freshness, validates options chain quality, detects arbitrage violations, and produces a structured quality report with configurable thresholds. Weekend-aware staleness prevents false alerts.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Phase 1 integration verification checkpoint</name>
  <files>
    tests/
  </files>
  <action>
    Human verifies the complete Phase 1 data infrastructure and options math engine:
    1. Project scaffold with uv and all dependencies
    2. Parquet data lake with append-only hive partitioning
    3. Feature store with point-in-time correct queries (lookahead prevention)
    4. Futures, options, and COT ingestion pipelines
    5. SVI volatility surface calibration
    6. Breeden-Litzenberger density extraction with graceful degradation
    7. Implied moments (mean, variance, skew, kurtosis)
    8. Greeks flow aggregation (GEX, vanna, charm)
    9. Data quality monitoring with staleness detection

    Steps to verify:
    1. Run the full test suite: `uv run pytest tests/ -v --tb=short`
       - All tests should pass (expect 20+ tests across all modules)
    2. Verify the critical lookahead prevention test passes:
       - Look for test_point_in_time_prevents_lookahead in output
    3. Verify graceful degradation tests pass (look for "degraded" or "sparse" in test names)
    4. Spot check imports: `uv run python -c "from hydra.signals.options_math.surface import calibrate_svi; from hydra.signals.options_math.density import extract_density; from hydra.signals.options_math.greeks import compute_greeks_flow; from hydra.data.quality import DataQualityMonitor; print('All Phase 1 modules import OK')"`

    Type "approved" if all tests pass and modules import correctly, or describe any issues to fix.
  </action>
  <verify>
    `uv run pytest tests/ -v --tb=short` -- all tests pass, 20+ tests across all modules
  </verify>
  <done>
    Human has verified the full Phase 1 integration: all tests pass, all modules import, lookahead prevention works, graceful degradation works, data quality monitoring works.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- full suite passes
2. DataQualityMonitor detects stale data correctly
3. Weekend-aware staleness works (Friday data not stale on Monday)
4. Options quality check detects arbitrage violations
5. Overall quality report produces structured output for all data sources
6. Human confirms full Phase 1 integration is working
</verification>

<success_criteria>
- Data quality monitoring with configurable staleness thresholds for all data sources
- Weekend-aware staleness detection (no false alerts on weekends)
- Options chain quality validation (liquid strike count, call price monotonicity, put-call parity)
- Structured quality report with overall status
- Full Phase 1 test suite passes
- Human verification of end-to-end integration
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-infrastructure-options-math-engine/01-06-SUMMARY.md`
</output>
