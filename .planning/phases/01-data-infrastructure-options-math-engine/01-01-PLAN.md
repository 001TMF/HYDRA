---
phase: 01-data-infrastructure-options-math-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/hydra/__init__.py
  - src/hydra/data/__init__.py
  - src/hydra/data/store/__init__.py
  - src/hydra/data/store/parquet_lake.py
  - src/hydra/data/store/feature_store.py
  - src/hydra/data/ingestion/__init__.py
  - src/hydra/signals/__init__.py
  - src/hydra/signals/options_math/__init__.py
  - src/hydra/config/default.yaml
  - tests/conftest.py
  - tests/test_parquet_lake.py
  - tests/test_feature_store.py
autonomous: true
requirements:
  - DATA-04
  - DATA-05

must_haves:
  truths:
    - "Parquet lake writes data and reads it back with hive partitioning intact"
    - "Feature store prevents lookahead bias via available_at filtering"
    - "COT data written with Tuesday as_of and Friday available_at is NOT returned when querying Wednesday"
    - "Feature store returns latest available feature value for a given query time"
    - "Project installs cleanly with uv and all tests pass"
  artifacts:
    - path: "src/hydra/data/store/parquet_lake.py"
      provides: "Append-only Parquet data lake with hive partitioning"
      min_lines: 60
    - path: "src/hydra/data/store/feature_store.py"
      provides: "Point-in-time correct feature store with as_of/available_at semantics"
      min_lines: 80
    - path: "pyproject.toml"
      provides: "Project configuration with all Phase 1 dependencies"
      contains: "pyarrow"
    - path: "tests/test_feature_store.py"
      provides: "Lookahead bias prevention tests"
      min_lines: 40
  key_links:
    - from: "src/hydra/data/store/feature_store.py"
      to: "SQLite database"
      via: "sqlite3 connection with as_of/available_at schema"
      pattern: "available_at.*<="
    - from: "src/hydra/data/store/parquet_lake.py"
      to: "Parquet files on disk"
      via: "pyarrow.dataset.write_dataset with hive partitioning"
      pattern: "write_dataset"
---

<objective>
Scaffold the HYDRA project structure, create the Parquet data lake and point-in-time feature store, and establish the testing foundation.

Purpose: Every data pipeline and options math module depends on having a place to persist raw data (Parquet lake) and serve features without lookahead bias (feature store). This plan creates the foundation that all subsequent plans build on.

Output: A working Python project with uv, a Parquet lake that supports append-only writes with hive partitioning, a SQLite feature store with as_of/available_at semantics, and passing tests that prove lookahead bias prevention works.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-infrastructure-options-math-engine/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scaffold project structure with uv and core dependencies</name>
  <files>
    pyproject.toml
    src/hydra/__init__.py
    src/hydra/data/__init__.py
    src/hydra/data/store/__init__.py
    src/hydra/data/ingestion/__init__.py
    src/hydra/signals/__init__.py
    src/hydra/signals/options_math/__init__.py
    src/hydra/config/default.yaml
    tests/__init__.py
    tests/conftest.py
  </files>
  <action>
    Create the project using uv:
    1. Run `uv init` if pyproject.toml doesn't exist, then edit pyproject.toml to set:
       - name = "hydra"
       - python = ">=3.11"
       - Core dependencies: numpy>=1.26, scipy>=1.12, pyarrow>=15.0, structlog>=24.1, apscheduler>=3.10
       - Data source dependencies: databento, cot-reports
       - Dev dependencies group: pytest, pytest-asyncio, ruff, mypy
    2. Create the directory structure matching the research architecture:
       - src/hydra/ with __init__.py (version = "0.1.0")
       - src/hydra/data/store/ (for parquet_lake.py, feature_store.py)
       - src/hydra/data/ingestion/ (empty __init__.py, base.py placeholder for Plan 02)
       - src/hydra/signals/options_math/ (empty __init__.py for Plan 03+)
       - src/hydra/config/default.yaml with all configurable thresholds from research:
         - staleness_thresholds: futures=1, options=1, cot_days=7
         - quality: min_liquid_strikes=8, max_spread_pct=0.20, min_oi=50
         - markets: default target "HE" (lean hogs), contract_multiplier=400, cftc_code="054642"
    3. Create tests/__init__.py and tests/conftest.py with shared fixtures:
       - tmp_parquet_dir fixture (tmp_path based)
       - tmp_feature_db fixture (tmp_path / "features.db")
    4. Run `uv sync` to install all dependencies and verify the environment works.
    5. Run `uv run python -c "import hydra; print(hydra.__version__)"` to verify the package is importable.

    Do NOT install QuantLib yet -- it's optional and only needed if SVI proves insufficient (per research recommendation).
  </action>
  <verify>
    - `uv run python -c "import numpy, scipy, pyarrow, structlog; print('OK')"` prints OK
    - `uv run python -c "import hydra; print(hydra.__version__)"` prints 0.1.0
    - All __init__.py files exist in the directory tree
    - default.yaml contains min_liquid_strikes, max_spread_pct, min_oi keys
  </verify>
  <done>
    Project is installable via uv, all core dependencies import successfully, directory structure matches research architecture, config file contains all Phase 1 thresholds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Parquet lake and feature store with lookahead-prevention tests</name>
  <files>
    src/hydra/data/store/parquet_lake.py
    src/hydra/data/store/feature_store.py
    tests/test_parquet_lake.py
    tests/test_feature_store.py
  </files>
  <action>
    Implement the two core storage modules following research patterns exactly:

    **parquet_lake.py** (Pattern 3 from research):
    - ParquetLake class with __init__(base_path: str | Path)
    - write(table: pa.Table, data_type: str, market: str, date: datetime) -> Path
      - Hive partitioning: data_type / market / year / month
      - Unique basename_template with UUID to prevent overwrites (append-only)
      - Uses pyarrow.dataset.write_dataset with existing_data_behavior="overwrite_or_ignore"
    - read(data_type: str, market: str, start: datetime = None, end: datetime = None) -> pa.Table
      - Reads from hive-partitioned dataset with optional date filters
    - All timestamps stored as UTC strings (per research anti-pattern: no naive datetimes)

    **feature_store.py** (Pattern 2 from research):
    - FeatureStore class with __init__(db_path: str | Path)
    - Schema: (market TEXT, feature_name TEXT, as_of TEXT, available_at TEXT, value REAL, quality TEXT DEFAULT 'normal')
    - PRIMARY KEY on (market, feature_name, as_of)
    - Index on (market, feature_name, available_at) for point-in-time queries
    - write_feature(market, feature_name, as_of: datetime, available_at: datetime, value: float, quality: str = "normal")
    - get_features_at(market: str, query_time: datetime) -> dict[str, float]
      - CRITICAL: Filters by available_at <= query_time, NOT as_of
      - Returns latest available value per feature name
    - get_feature_history(market: str, feature_name: str, start: datetime, end: datetime) -> list[dict]
      - For debugging/analysis -- returns all values in range
    - close() method for cleanup

    **tests/test_parquet_lake.py:**
    - test_write_and_read_roundtrip: write a table, read it back, verify data matches
    - test_append_only_creates_unique_files: write same date twice, verify two parquet files exist
    - test_hive_partitioning_structure: verify directory structure matches data_type=X/market=Y/year=Z/month=W

    **tests/test_feature_store.py:**
    - test_write_and_read_feature: basic write/read roundtrip
    - test_point_in_time_prevents_lookahead: Write COT feature with as_of=Tuesday, available_at=Friday. Query on Wednesday -> NOT returned. Query on Saturday -> returned.
    - test_latest_value_returned: Write two values for same feature at different as_of dates. Query returns the latest one that's available.
    - test_quality_flag_stored: Write with quality="degraded", verify it's stored correctly.

    Use structlog for any logging within the modules. All datetime parameters must be timezone-aware UTC.
  </action>
  <verify>
    - `uv run pytest tests/test_parquet_lake.py tests/test_feature_store.py -v` -- all tests pass
    - Specifically verify the lookahead test: COT Tuesday data not returned on Wednesday query
  </verify>
  <done>
    Parquet lake writes and reads with hive partitioning. Feature store serves point-in-time correct queries. The critical lookahead prevention test (COT Tuesday/Friday timing) passes. Both modules have test coverage.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- all tests pass
2. `uv run python -c "from hydra.data.store.parquet_lake import ParquetLake; print('ParquetLake OK')"` -- imports
3. `uv run python -c "from hydra.data.store.feature_store import FeatureStore; print('FeatureStore OK')"` -- imports
4. Feature store lookahead test specifically passes
</verification>

<success_criteria>
- Project scaffolded with all Phase 1 dependencies installable via uv
- Parquet lake supports append-only writes with hive partitioning
- Feature store prevents lookahead bias via available_at filtering (proven by test)
- All tests green
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-infrastructure-options-math-engine/01-01-SUMMARY.md`
</output>
