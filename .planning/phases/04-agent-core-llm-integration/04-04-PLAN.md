---
phase: 04-agent-core-llm-integration
plan: 04
type: execute
wave: 3
depends_on: [04-01, 04-02, 04-03]
files_modified:
  - src/hydra/agent/loop.py
  - src/hydra/agent/coordinator.py
  - src/hydra/agent/arena.py
  - src/hydra/agent/heads/__init__.py
  - src/hydra/agent/heads/base.py
  - tests/test_agent_loop.py
  - tests/test_coordinator.py
autonomous: true
requirements: [AGNT-01, AGNT-11, AGNT-15]

must_haves:
  truths:
    - "Agent loop runs full observe-diagnose-hypothesize-experiment-evaluate cycle autonomously"
    - "Each step of the loop is logged to the experiment journal"
    - "Agent loop checks autonomy level before each action and respects gating"
    - "Agent loop uses rollback trigger and promotion evaluator to manage model lifecycle"
    - "Head Coordinator dispatches diagnosis to multiple heads and collects competing hypotheses"
    - "Arena tournament ranks hypotheses by fitness score and advances top-K to full evaluation"
    - "Abstract head interface defines the contract all heads must implement"
  artifacts:
    - path: "src/hydra/agent/loop.py"
      provides: "Main agent loop state machine"
      exports: ["AgentLoop", "AgentPhase", "AgentState"]
    - path: "src/hydra/agent/coordinator.py"
      provides: "Head Coordinator: dispatch + collection"
      exports: ["HeadCoordinator"]
    - path: "src/hydra/agent/arena.py"
      provides: "Tournament ranking of competing hypotheses"
      exports: ["Arena", "ArenaResult"]
    - path: "src/hydra/agent/heads/base.py"
      provides: "Abstract head interface"
      exports: ["BaseHead"]
  key_links:
    - from: "src/hydra/agent/loop.py"
      to: "src/hydra/sandbox/observer.py"
      via: "DriftObserver.get_full_report() in observe phase"
      pattern: "observer\\.get_full_report|DriftReport"
    - from: "src/hydra/agent/loop.py"
      to: "src/hydra/sandbox/journal.py"
      via: "ExperimentJournal.log() after each experiment"
      pattern: "journal\\.log|ExperimentRecord"
    - from: "src/hydra/agent/coordinator.py"
      to: "src/hydra/agent/heads/base.py"
      via: "Calls head.generate_hypotheses() on each registered head"
      pattern: "generate_hypotheses"
    - from: "src/hydra/agent/arena.py"
      to: "src/hydra/sandbox/replay.py"
      via: "MarketReplayEngine for hypothesis evaluation"
      pattern: "MarketReplayEngine|replay"
    - from: "src/hydra/agent/arena.py"
      to: "src/hydra/agent/promotion.py"
      via: "PromotionEvaluator for 3-of-5 window final evaluation"
      pattern: "PromotionEvaluator"
---

<objective>
Build the agent loop state machine, head coordinator, arena tournament, and abstract head interface -- the orchestration layer that ties all Phase 4 components together.

Purpose: The agent loop is the beating heart of HYDRA's self-healing system. It orchestrates the observe-diagnose-hypothesize-experiment-evaluate cycle. The coordinator dispatches to multiple heads. The arena ranks competing hypotheses. Together they implement the multi-headed autonomous improvement engine.

Output: Agent loop, coordinator, arena, and abstract head base class with tests verifying the full cycle.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
@.planning/phases/04-agent-core-llm-integration/04-01-SUMMARY.md
@.planning/phases/04-agent-core-llm-integration/04-02-SUMMARY.md
@.planning/phases/04-agent-core-llm-integration/04-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create agent loop state machine with journal logging</name>
  <files>
    src/hydra/agent/loop.py
    src/hydra/agent/heads/__init__.py
    src/hydra/agent/heads/base.py
    tests/test_agent_loop.py
  </files>
  <action>
    **Create `src/hydra/agent/heads/__init__.py`**: Export BaseHead.

    **Create `src/hydra/agent/heads/base.py`**:
    - `BaseHead(ABC)`:
      - `name: str` (abstract property) -- unique head identifier (e.g., "technical", "research", "structural")
      - `generate_hypotheses(self, diagnosis: DiagnosisResult, journal: ExperimentJournal, max_hypotheses: int = 3) -> list[Hypothesis]`: Abstract method. Each head receives the diagnosis and journal (for shared memory of past experiments) and returns hypotheses.
      - `head_type: str` (abstract property) -- "technical", "research", or "structural"

    **Create `src/hydra/agent/loop.py`**: Follow research Pattern 1.
    - `AgentPhase(Enum)`: IDLE, OBSERVE, DIAGNOSE, HYPOTHESIZE, EXPERIMENT, EVALUATE
    - `AgentState` dataclass: phase (AgentPhase), drift_report (DriftReport | None), diagnosis (DiagnosisResult | None), hypotheses (list[Hypothesis] | None), experiment_results (list[ExperimentResult] | None), cycle_count (int = 0)
    - `AgentLoop`:
      - `__init__(self, observer: DriftObserver, diagnostician: Diagnostician, coordinator: "HeadCoordinator", arena: "Arena", journal: ExperimentJournal, autonomy: AutonomyLevel = AutonomyLevel.SUPERVISED, rollback_trigger: HysteresisRollbackTrigger | None = None)`:
        Store all components. Initialize state at IDLE.
      - `step(self) -> AgentPhase`: Execute one step of the loop state machine.
        - **IDLE -> OBSERVE**: Check autonomy for "observe". Call observer.get_full_report(). If needs_diagnosis, transition to DIAGNOSE. Else stay IDLE.
        - **OBSERVE -> DIAGNOSE**: Check autonomy for "diagnose". Call diagnostician.diagnose(drift_report). Store diagnosis. Transition to HYPOTHESIZE.
        - **DIAGNOSE -> HYPOTHESIZE**: Check autonomy for "hypothesize". Call coordinator.dispatch(diagnosis, journal). Collect hypotheses. If empty, transition back to IDLE. Else transition to EXPERIMENT.
        - **HYPOTHESIZE -> EXPERIMENT**: Check autonomy for "experiment". Call arena.evaluate_hypotheses(hypotheses). Store results. Transition to EVALUATE.
        - **EXPERIMENT -> EVALUATE**: Check promotion via arena.check_promotion(). If promoted, log to journal with promotion_decision="promoted". If rollback_trigger fires, execute rollback. Log to journal. Increment cycle_count. Transition to IDLE.
      - `run_cycle(self) -> dict`: Convenience method that calls step() repeatedly until returning to IDLE. Returns summary dict with cycle_count, diagnosis, hypotheses_generated, promoted, rolled_back.
      - On any step where autonomy check fails, log the blocked action and return current phase (do not advance).

    **Create `tests/test_agent_loop.py`**:
    1. test_idle_to_observe_no_drift: Mock observer with needs_diagnosis=False. step() from IDLE returns IDLE (no drift, no progression).
    2. test_full_cycle_with_mocks: Mock all components. Start at IDLE. Call run_cycle(). Verify it progresses through all phases and returns to IDLE. Verify journal.log() called.
    3. test_autonomy_blocks_experiment: Set autonomy=SUPERVISED. Mock drift + diagnosis. Verify step() from HYPOTHESIZE stays at HYPOTHESIZE (experiment blocked).
    4. test_autonomy_blocks_at_lockdown: Set autonomy=LOCKDOWN. Verify step() from IDLE stays at IDLE (observe blocked).
    5. test_cycle_count_increments: Run two full cycles. Verify state.cycle_count == 2.
    6. test_empty_hypotheses_returns_to_idle: Coordinator returns empty list. Verify loop goes back to IDLE without entering EXPERIMENT.

    Mock all dependencies (observer, diagnostician, coordinator, arena, journal). Use simple return values: observer.get_full_report() returns a mock DriftReport with needs_diagnosis=True, diagnostician.diagnose() returns a mock DiagnosisResult, coordinator.dispatch() returns [mock_hypothesis], arena.evaluate_hypotheses() returns mock results, arena.check_promotion() returns a mock PromotionResult.
  </action>
  <verify>Run `python -m pytest tests/test_agent_loop.py -v` -- all 6 tests pass.</verify>
  <done>Agent loop state machine progresses through full cycle. Autonomy gating works at each step. Journal logging captures each experiment. Cycle counter tracks iterations. 6 tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create Head Coordinator and Arena tournament</name>
  <files>
    src/hydra/agent/coordinator.py
    src/hydra/agent/arena.py
    tests/test_coordinator.py
  </files>
  <action>
    **Create `src/hydra/agent/coordinator.py`**:
    - `HeadCoordinator`:
      - `__init__(self, deduplicator: HypothesisDeduplicator | None = None)`: Optional dedup engine.
      - `heads: list[BaseHead]`: Registered heads.
      - `register_head(self, head: BaseHead) -> None`: Add head to the roster.
      - `dispatch(self, diagnosis: DiagnosisResult, journal: ExperimentJournal, max_per_head: int = 3) -> list[Hypothesis]`: Main dispatch method.
        1. For each head in self.heads: call head.generate_hypotheses(diagnosis, journal, max_per_head). Catch exceptions per head (one head's failure should not block others). Log warnings on failure.
        2. Collect all hypotheses into a flat list.
        3. If deduplicator is provided: filter through dedup. For each hypothesis, call deduplicator.should_reject(hypothesis.description, hypothesis.mutation_type.value). Reject duplicates and cooldown violations. Log rejected hypotheses.
        4. Return surviving hypotheses.
      - `get_head(self, name: str) -> BaseHead | None`: Lookup head by name.

    **Create `src/hydra/agent/arena.py`**:
    - `ArenaResult` dataclass: hypothesis (Hypothesis), fitness_score (float), rank (int)
    - `Arena`:
      - `__init__(self, evaluator: CompositeEvaluator, experiment_runner: ExperimentRunner, promotion_evaluator: PromotionEvaluator, top_k: int = 3)`: Stores components.
      - `evaluate_hypotheses(self, hypotheses: list[Hypothesis], base_config: dict | None = None) -> list[ArenaResult]`: Tournament evaluation.
        1. For each hypothesis: call experiment_runner.run_experiment(hypothesis, base_config or {}). Collect ExperimentResults.
        2. For successful experiments: score via evaluator (create a mock backtest-compatible object from experiment metrics if needed, or use evaluator.score() directly on the metrics dict).
        3. Rank all successful experiments by fitness_score descending.
        4. Return ranked ArenaResult list (top_k or all if fewer).
      - `check_promotion(self, arena_results: list[ArenaResult], champion_fitness: float, replay_engine: "MarketReplayEngine | None" = None) -> PromotionResult | None`: For the top-ranked ArenaResult, run full 5-window promotion evaluation using PromotionEvaluator.
        - If replay_engine is provided: call `replay_engine.get_evaluation_windows(n=5)` to get 5 time-sliced data windows. For each window, score the candidate and champion via `self.evaluator`. Collect 5 candidate scores and 5 champion scores. Pass to `self.promotion_evaluator.evaluate(candidate_scores, champion_scores)` and return the PromotionResult.
        - If replay_engine is None (e.g., in tests): fall back to a single-score comparison -- construct candidate_scores as [arena_results[0].fitness_score] * 5 and champion_scores as [champion_fitness] * 5, pass to promotion_evaluator.evaluate(). This allows tests to use mock data without a full replay engine, while production code always provides the replay engine for proper 5-window evaluation.
        - Return None if arena_results is empty.

    **Create `tests/test_coordinator.py`**:
    1. test_dispatch_collects_from_all_heads: Register 2 mock heads returning 1 hypothesis each. Verify dispatch returns 2 hypotheses.
    2. test_dispatch_dedup_filters_duplicates: Register 2 mock heads returning similar hypotheses. Provide deduplicator mock that rejects the second. Verify only 1 survives.
    3. test_dispatch_one_head_failure_continues: Register 2 heads, first raises Exception. Verify second head's hypotheses still returned (no crash).
    4. test_arena_ranks_by_fitness: Provide 3 hypotheses with mock experiment results of varying fitness. Verify ArenaResult list is sorted descending by fitness_score.
    5. test_arena_respects_top_k: top_k=2 with 3 hypotheses. Verify only 2 results returned.
    6. test_arena_handles_failed_experiments: One hypothesis fails (experiment returns success=False). Verify it is excluded from results.
    7. test_check_promotion_returns_result: Top candidate beats champion -> returns PromotionResult with promoted=True from PromotionEvaluator.
    8. test_check_promotion_with_mock_replay: Provide a mock MarketReplayEngine with get_evaluation_windows(n=5) returning 5 window objects. Verify Arena.check_promotion calls replay_engine, scores each window via evaluator, and passes 5-element score lists to PromotionEvaluator.evaluate().

    Mock ExperimentRunner.run_experiment to return ExperimentResult with synthetic metrics. Mock CompositeEvaluator.score() to return configurable FitnessScore. Mock MarketReplayEngine.get_evaluation_windows for test_check_promotion_with_mock_replay.
  </action>
  <verify>Run `python -m pytest tests/test_coordinator.py -v` -- all 8 tests pass.</verify>
  <done>Head Coordinator dispatches to registered heads, deduplicates, and handles per-head failures gracefully. Arena evaluates hypotheses through experiment runner, ranks by fitness, and checks promotion using PromotionEvaluator with 5-window evaluation (via MarketReplayEngine when available, single-score fallback for tests). 8 tests pass.</done>
</task>

</tasks>

<verification>
1. `python -c "from hydra.agent.loop import AgentLoop, AgentPhase; from hydra.agent.coordinator import HeadCoordinator; from hydra.agent.arena import Arena; from hydra.agent.heads.base import BaseHead"` succeeds
2. `python -m pytest tests/test_agent_loop.py tests/test_coordinator.py -v` -- all 14 tests pass
3. Agent loop progresses through all 6 phases
4. Coordinator collects from multiple heads and filters duplicates
5. Arena ranks hypotheses and identifies promotion candidates
6. Arena.check_promotion uses PromotionEvaluator with 5-window evaluation via MarketReplayEngine
</verification>

<success_criteria>
Agent loop state machine runs full observe-diagnose-hypothesize-experiment-evaluate cycle. Head Coordinator dispatches to multiple heads with per-head failure isolation and dedup filtering. Arena tournament ranks hypotheses by fitness and evaluates top-K for promotion using PromotionEvaluator with actual 5-window evaluation (MarketReplayEngine provides time-sliced windows). Abstract BaseHead interface defines the contract for specialized heads. All 14 tests pass.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-04-SUMMARY.md`
</output>
