---
phase: 04-agent-core-llm-integration
plan: 04
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/hydra/agent/experiment_runner.py
  - src/hydra/agent/_train_candidate.py
  - src/hydra/agent/dedup.py
  - src/hydra/agent/budget.py
  - tests/agent/__init__.py
  - tests/agent/test_experiment_runner.py
  - tests/agent/test_dedup.py
  - tests/agent/test_budget.py
autonomous: true
requirements: [AGNT-04, AGNT-09]

must_haves:
  truths:
    - "Experiment runner trains a candidate model in a subprocess with configurable timeout and returns structured results"
    - "Subprocess isolation prevents candidate training from affecting the main agent process (memory leaks, crashes)"
    - "Semantic dedup rejects hypotheses with cosine similarity > 0.85 to any recently tried hypothesis"
    - "Mutation budgets limit the number of experiments per category per cycle"
    - "Cooldown timers prevent retrying the same mutation type within a configurable window"
    - "Dedup and budget enforcement work entirely locally with no LLM calls"
  artifacts:
    - path: "src/hydra/agent/experiment_runner.py"
      provides: "Subprocess-isolated candidate training with timeout"
      exports: ["ExperimentRunner", "ExperimentResult", "ExperimentError"]
    - path: "src/hydra/agent/dedup.py"
      provides: "Semantic deduplication via sentence-transformers embeddings"
      exports: ["HypothesisDeduplicator"]
    - path: "src/hydra/agent/budget.py"
      provides: "Mutation budgets and cooldown timers"
      exports: ["MutationBudget", "BudgetConfig"]
  key_links:
    - from: "src/hydra/agent/dedup.py"
      to: "src/hydra/sandbox/journal.py"
      via: "deduplicator loads recent hypotheses from experiment journal"
      pattern: "journal\\.query"
    - from: "src/hydra/agent/experiment_runner.py"
      to: "src/hydra/agent/_train_candidate.py"
      via: "subprocess invokes _train_candidate stub; full replay integration deferred to 04-05 or Phase 5"
      pattern: "subprocess\\.run.*_train_candidate"
---

<objective>
Build the experiment runner (subprocess isolation), semantic deduplication, and mutation budget/cooldown system.

Purpose: AGNT-04 requires candidate training in subprocess isolation with configurable timeout. AGNT-09 requires three layers of defense against degenerate experiment loops: (1) semantic dedup via embeddings, (2) per-category mutation budgets, (3) cooldown timers. All three components are pure computation -- no LLM dependency. The dedup uses a local sentence-transformers model (all-MiniLM-L6-v2, 22M params, runs on CPU in <10ms).

Output: Three modules in `src/hydra/agent/` with comprehensive test coverage.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
@src/hydra/sandbox/journal.py
@src/hydra/sandbox/replay.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Experiment runner with subprocess isolation and timeout (TDD)</name>
  <files>
    src/hydra/agent/experiment_runner.py
    src/hydra/agent/_train_candidate.py
    tests/agent/__init__.py
    tests/agent/test_experiment_runner.py
  </files>
  <action>
**tests/agent/__init__.py** -- Create idempotently (empty file). This plan runs in parallel with 04-01, 04-02, and 04-03. Use `open("tests/agent/__init__.py", "a").close()` or `Path("tests/agent/__init__.py").touch()` so whichever plan runs first wins and subsequent plans are no-ops.

**experiment_runner.py** -- Subprocess-isolated candidate training (AGNT-04):

- `ExperimentError(Exception)`: raised on timeout, crash, or invalid output from subprocess.
- `ExperimentResult(dataclass)`: success (bool), fitness_score (float | None), metrics (dict), config_used (dict), duration_seconds (float), error_message (str | None), stdout (str), stderr (str).
- `ExperimentRunner` class:
  - `__init__(self, timeout_seconds: int = 300, python_executable: str | None = None)`: Store timeout config. Default python_executable to `sys.executable` (the current interpreter).
  - `run(self, hypothesis: "Hypothesis", base_config: dict) -> ExperimentResult`: Main entry point.
    1. Merge `hypothesis.config_diff` into `base_config` to produce the candidate config.
    2. Write candidate config to a temporary JSON file (use `tempfile.NamedTemporaryFile`).
    3. Call `subprocess.run([python_executable, "-m", "hydra.agent._train_candidate", config_path], timeout=timeout_seconds, capture_output=True, text=True)`.
    4. Parse stdout as JSON for the result metrics. If parsing fails, return ExperimentResult with success=False and error_message.
    5. Clean up temp file.
    6. Wrap `subprocess.TimeoutExpired` into ExperimentResult(success=False, error_message="Training timed out after N seconds").
    7. Wrap `subprocess.CalledProcessError` into ExperimentResult(success=False) with stderr captured.
  - `_merge_config(self, base: dict, diff: dict) -> dict`: Deep merge diff into base. For numeric expressions like "current * 0.5", resolve them using the base value. Fall back to raw value if expression is unresolvable.

- `_train_candidate` module stub (`src/hydra/agent/_train_candidate.py`): A minimal entrypoint that reads config from argv[1], prints a JSON result to stdout. For now, this is a STUB that returns a dummy result -- the full training integration wires into BaselineModel + MarketReplayEngine in Plan 05 when the agent loop is assembled. The stub exists so the subprocess call has a real target for testing.

**Important**: Do NOT add sentence-transformers yet (that's Task 2). Only install subprocess-related deps (none needed -- subprocess is stdlib).

**test_experiment_runner.py**:
- Test run() with a config that the stub can process -- verify ExperimentResult returned with success=True
- Test timeout: mock subprocess.run to raise TimeoutExpired -- verify ExperimentResult(success=False, error_message contains "timed out")
- Test crash: mock subprocess.run to raise CalledProcessError -- verify ExperimentResult(success=False) with stderr
- Test invalid output: mock subprocess returning non-JSON stdout -- verify ExperimentResult(success=False, error_message about parse failure)
- Test _merge_config: base={"lr": 0.1}, diff={"lr": 0.05} produces {"lr": 0.05}
- Test _merge_config with expression: base={"lr": 0.1}, diff={"lr": "current * 0.5"} produces {"lr": 0.05}
- Test cleanup: temp file is deleted after run (success or failure)
  </action>
  <verify>
`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/agent/test_experiment_runner.py -v` -- all tests pass.
`python -c "from hydra.agent.experiment_runner import ExperimentRunner, ExperimentResult, ExperimentError; print('import OK')"` succeeds.
  </verify>
  <done>Experiment runner isolates candidate training in subprocess with configurable timeout. Timeout and crash scenarios return structured ExperimentResult with error details. Config merging resolves numeric expressions. Temp files cleaned up. All tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Semantic dedup and mutation budgets with cooldowns (TDD)</name>
  <files>
    src/hydra/agent/dedup.py
    src/hydra/agent/budget.py
    tests/agent/test_dedup.py
    tests/agent/test_budget.py
  </files>
  <action>
**NOTE**: Do NOT run `uv add sentence-transformers` here. All Wave 1 dependency additions are consolidated in 04-01 Task 2 to prevent concurrent `uv add` race conditions. sentence-transformers is already added by 04-01.

**dedup.py** -- Semantic deduplication (AGNT-09 layer 1):
- `HypothesisDeduplicator` class:
  - `__init__(self, similarity_threshold: float = 0.85, model_name: str = "all-MiniLM-L6-v2")`: Load the sentence-transformers model. Store threshold.
  - `is_duplicate(self, description: str) -> tuple[bool, float]`: Check if hypothesis description is too similar to any recently registered hypothesis. Return (is_dup, max_similarity_score). If no recent embeddings, return (False, 0.0).
  - `register(self, description: str) -> None`: Add hypothesis embedding to the recent memory.
  - `load_from_journal(self, journal: ExperimentJournal, days: int = 30) -> None`: Query journal for experiments in the last N days, register each hypothesis description. This pre-loads the dedup memory from persistent storage.
  - `clear(self) -> None`: Reset the in-memory recent embeddings.
  - `_recent_embeddings: list[np.ndarray]`: In-memory list of embeddings.
  - `_recent_descriptions: list[str]`: Corresponding descriptions for debugging.

Internal implementation: use `self.model.encode([description])` to get 384-dim embedding. Cosine similarity via `np.dot(a, b) / (norm(a) * norm(b))`. Vectorized comparison against all recent embeddings.

**budget.py** -- Mutation budgets and cooldown timers (AGNT-09 layers 2 and 3):
- `BudgetConfig(dataclass)`:
  - max_experiments_per_cycle (int, default 5): Total experiments allowed per agent cycle.
  - max_per_category (dict[str, int], default {"hyperparameter": 3, "feature_add": 2, "feature_remove": 2, "feature_engineering": 2, "ensemble_method": 1, "prediction_target": 1}): Per-category limits per cycle.
  - cooldown_days (dict[str, int], default {"hyperparameter": 3, "feature_add": 7, "feature_remove": 7, "ensemble_method": 14}): Minimum days between trying same mutation type after a rejection.
- `MutationBudget` class:
  - `__init__(self, config: BudgetConfig | None = None)`: Store config, initialize counters.
  - `can_run(self, mutation_type: str) -> tuple[bool, str]`: Check if budget and cooldown allow this mutation type. Return (allowed, reason). Checks: (1) total experiments this cycle < max_experiments_per_cycle, (2) this category count < max_per_category, (3) cooldown not active for this type.
  - `record_experiment(self, mutation_type: str, promoted: bool) -> None`: Increment counters. If not promoted, start cooldown timer for that category.
  - `reset_cycle(self) -> None`: Reset per-cycle counters (called at start of each agent cycle). Cooldown timers are NOT reset -- they persist across cycles.
  - `load_cooldowns_from_journal(self, journal: ExperimentJournal) -> None`: Scan recent rejected experiments to reconstruct cooldown state. For each rejected experiment within cooldown_days of today, mark that category as on cooldown.
  - `_cycle_counts: dict[str, int]`: Per-category experiment count this cycle.
  - `_total_this_cycle: int`: Total experiment count this cycle.
  - `_cooldown_until: dict[str, datetime]`: Category -> earliest datetime when that category can be retried.

**test_dedup.py**:
- Test is_duplicate with no history returns (False, 0.0)
- Test identical strings are detected as duplicate (similarity ~1.0)
- Test semantically similar strings are detected ("reduce learning rate by half" vs "halve the learning rate")
- Test clearly different strings are NOT duplicates ("reduce learning rate" vs "add new features from USDA data")
- Test threshold: 0.85 catches near-duplicates, 0.99 only catches exact matches
- Test register + is_duplicate flow: register "reduce lr", then "reduce learning rate" is_duplicate
- Test clear() resets memory
- Mock the journal in load_from_journal to verify it queries and registers descriptions

**test_budget.py**:
- Test can_run returns (True, ...) when budget is fresh
- Test can_run returns (False, "budget exceeded") after max_experiments_per_cycle reached
- Test per-category limit: 3 hyperparameter experiments allowed, 4th blocked
- Test cooldown: after rejection, category is blocked for cooldown_days
- Test cooldown does NOT apply to promoted experiments
- Test reset_cycle resets per-cycle counts but NOT cooldowns
- Test custom BudgetConfig values work
- Test load_cooldowns_from_journal reconstructs cooldown state from rejected experiments
  </action>
  <verify>
`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/agent/test_dedup.py tests/agent/test_budget.py -v` -- all tests pass.
`python -c "from hydra.agent.dedup import HypothesisDeduplicator; print('dedup OK')"` succeeds.
`python -c "from hydra.agent.budget import MutationBudget, BudgetConfig; b = MutationBudget(); assert b.can_run('hyperparameter')[0]; print('budget OK')"` succeeds.
  </verify>
  <done>Three-layer defense against degenerate loops: semantic dedup (cosine > 0.85 rejects), per-category mutation budgets, and cooldown timers after rejections. All run locally with zero LLM calls. Dedup uses all-MiniLM-L6-v2 (22M params, CPU). Budget integrates with journal for persistent cooldown state. All tests pass.</done>
</task>

</tasks>

<verification>
- `python -m pytest tests/agent/test_experiment_runner.py tests/agent/test_dedup.py tests/agent/test_budget.py -v` -- all pass
- Experiment runner: subprocess isolation with timeout, crash handling, config merging
- Dedup: cosine similarity > 0.85 rejects near-duplicate hypotheses
- Budget: per-cycle limits + per-category limits + cooldown timers
- All three work with zero LLM calls (pure computation / local model)
</verification>

<success_criteria>
Experiment isolation and loop-prevention guardrails are complete. Candidate training runs in a subprocess with configurable timeout (AGNT-04). Three independent mechanisms prevent degenerate experiment loops: semantic dedup, mutation budgets, and cooldown timers (AGNT-09). All tests pass. These modules will be wired into the agent loop in Plan 05.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-04-SUMMARY.md`
</output>
