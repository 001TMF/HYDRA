---
phase: 04-agent-core-llm-integration
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/hydra/agent/__init__.py
  - src/hydra/agent/llm/__init__.py
  - src/hydra/agent/llm/client.py
  - src/hydra/agent/llm/schemas.py
  - src/hydra/agent/llm/router.py
  - tests/agent/test_llm_client.py
  - tests/agent/__init__.py
  - tests/agent/llm/__init__.py
autonomous: true
requirements: [AGNT-05, AGNT-10]
user_setup:
  - service: together-ai
    why: "Optional -- LLM provider for DeepSeek-R1 and Qwen models. System works without it via rule-based fallback."
    env_vars:
      - name: TOGETHER_API_KEY
        source: "https://api.together.ai/settings/api-keys -- create account and copy API key"
      - name: DEEPSEEK_API_KEY
        source: "https://platform.deepseek.com/api_keys -- optional direct DeepSeek access"

must_haves:
  truths:
    - "LLM client calls an LLM provider with Pydantic-validated structured output and returns typed results"
    - "When all LLM providers fail, LLMUnavailableError is raised so callers can fall back to rule-based"
    - "Task-type router maps reasoning/classification/formatting tasks to cheapest capable model"
    - "System operates entirely rule-based when no API keys are configured (AGNT-10 default mode)"
    - "Daily cost tracking prevents exceeding configurable budget cap"
  artifacts:
    - path: "src/hydra/agent/llm/client.py"
      provides: "LLMClient with fallback chain and cost tracking"
      exports: ["LLMClient", "LLMUnavailableError"]
    - path: "src/hydra/agent/llm/schemas.py"
      provides: "Pydantic LLM wrappers (imports base types from hydra.agent.types)"
      exports: ["DiagnosisResultLLM", "HypothesisLLM", "ExperimentConfig"]
    - path: "src/hydra/agent/llm/router.py"
      provides: "Task-type to model routing"
      exports: ["TaskRouter", "TaskType"]
  key_links:
    - from: "src/hydra/agent/llm/schemas.py"
      to: "src/hydra/agent/types.py"
      via: "schemas.py imports DriftCategory, MutationType from shared types module (created by 04-03)"
      pattern: "from hydra\\.agent\\.types import"
    - from: "src/hydra/agent/llm/client.py"
      to: "src/hydra/agent/llm/router.py"
      via: "client calls router to get fallback chain per task type"
      pattern: "router\\.get_fallback_chain"
    - from: "src/hydra/agent/llm/client.py"
      to: "src/hydra/agent/llm/schemas.py"
      via: "response_model parameter is a Pydantic LLM wrapper schema"
      pattern: "response_model.*BaseModel"
---

<objective>
Build the optional LLM integration layer: multi-provider client with fallback chain, Pydantic I/O schemas, and task-type routing for cost optimization.

Purpose: AGNT-05 requires the LLM client with structured output and fallback chain. AGNT-10 requires the system works WITHOUT it. This plan builds the "turbo" -- the optional enhancement that the rest of the agent loop can use or ignore. The LLMUnavailableError is the key interface: callers catch it and fall back to rule-based logic.

Output: `src/hydra/agent/llm/` package with client, schemas, router, and tests.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pydantic schemas and task-type router</name>
  <files>
    src/hydra/agent/__init__.py
    src/hydra/agent/llm/__init__.py
    src/hydra/agent/llm/schemas.py
    src/hydra/agent/llm/router.py
    tests/agent/__init__.py
    tests/agent/llm/__init__.py
  </files>
  <action>
Create the `src/hydra/agent/` and `src/hydra/agent/llm/` packages.

**schemas.py** -- Pydantic wrappers for LLM I/O. Rather than re-defining core domain types, schemas.py imports the canonical types from `src/hydra/agent/types.py` (created by 04-03) and wraps them as Pydantic BaseModels for instructor/LLM use. This prevents the type incompatibility where 04-01 and 04-03 each define DiagnosisResult differently.

Since Wave 1 plans run in parallel, `types.py` may not exist when schemas.py is first authored. The executor must create it if absent (it will be identical to the version in 04-03/types.py). If 04-03 has already run, just import from it.

**Pattern for schemas.py**:
```python
# Import canonical types -- create types.py here if 04-03 has not run yet
from hydra.agent.types import DriftCategory, DiagnosisResult, MutationType, Hypothesis

# Pydantic wrappers add Field(description=...) for LLM schema extraction
class DiagnosisResultLLM(BaseModel):
    primary_cause: DriftCategory = Field(description="Root cause category of the model degradation")
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence in this diagnosis (0.0 to 1.0)")
    evidence: list[str] = Field(description="Specific signals from the drift report supporting this diagnosis")
    recommended_mutation_types: list[str] = Field(description="Which mutation types to try, in priority order")
    reasoning: str = Field(description="Step-by-step reasoning explaining this diagnosis")

class HypothesisLLM(BaseModel):
    mutation_type: MutationType = Field(description="Category of model change to attempt")
    description: str = Field(description="Human-readable description of the proposed change")
    config_diff: dict = Field(description="Key-value config changes to apply")
    expected_impact: str = Field(description="Predicted effect on model performance")
    testable_prediction: str = Field(description="Specific measurable outcome to validate")
    source: str = Field(default="llm", description="Origin: 'playbook' for rule-based, 'llm' for LLM-generated")
```

Also create `ExperimentConfig(BaseModel)`: hypothesis (HypothesisLLM), training_timeout_seconds (int, default 300), base_config (dict).

All Pydantic models use `Field(description=...)` so instructor can use them as schema hints for structured output.

**router.py** -- Task-type to model routing:
- `TaskType(str, Enum)`: reasoning, classification, formatting
- `ModelSpec(BaseModel)`: provider (str), model_id (str), cost_per_1m_input (float), cost_per_1m_output (float)
- `TaskRouter` class with `get_fallback_chain(task_type: TaskType) -> list[ModelSpec]`
- Default chains from research:
  - reasoning: DeepSeek-R1-0528 via Together -> Qwen3-235B via Together
  - classification: Qwen2.5-7B-Turbo via Together -> DeepSeek-V3.1 via Together
  - formatting: Qwen2.5-7B-Turbo via Together
- Router must be configurable (accept custom chains dict in constructor)

**`__init__.py` files**: Create empty `__init__.py` for `src/hydra/agent/`, `src/hydra/agent/llm/`, `tests/agent/`, `tests/agent/llm/`.
  </action>
  <verify>
`python -c "from hydra.agent.llm.schemas import DiagnosisResult, Hypothesis, DriftCategory, MutationType; print('schemas OK')"` succeeds.
`python -c "from hydra.agent.llm.router import TaskRouter, TaskType; r = TaskRouter(); chain = r.get_fallback_chain(TaskType.REASONING); assert len(chain) >= 2; print('router OK')"` succeeds.
  </verify>
  <done>All Pydantic schemas import and validate correctly. TaskRouter returns correct fallback chains for all task types. Package structure created.</done>
</task>

<task type="auto">
  <name>Task 2: LLM client with fallback chain, cost tracking, and tests</name>
  <files>
    src/hydra/agent/llm/client.py
    tests/agent/test_llm_client.py
  </files>
  <action>
**client.py** -- Multi-provider LLM client:
- `LLMUnavailableError(Exception)`: raised when all providers in the fallback chain fail. This is the key interface for AGNT-10 -- callers catch this and fall back to rule-based.
- `LLMConfig(BaseModel)`: together_api_key (str | None), deepseek_api_key (str | None), daily_budget (float, default 20.0), max_retries_per_provider (int, default 2)
- `LLMClient` class:
  - `__init__(self, config: LLMConfig, router: TaskRouter | None = None)`: Creates instructor-wrapped OpenAI clients for each configured provider. If NO API keys are provided, the client is in "disabled" mode -- any `call()` immediately raises LLMUnavailableError.
  - `call(self, response_model: type[T], messages: list[dict], task_type: TaskType = TaskType.REASONING) -> T`: Iterates through fallback chain from router. For each provider/model, calls `instructor.chat.completions.create(model=..., response_model=response_model, messages=messages, max_retries=config.max_retries_per_provider)`. On success, tracks cost and returns. On failure (any exception), continues to next. After exhausting chain, raises LLMUnavailableError.
  - `_track_cost(self, model_spec: ModelSpec, input_tokens: int, output_tokens: int)`: Adds estimated cost to `_daily_cost`. Approximate tokens from message length (input) and response (output).
  - `_check_budget(self) -> bool`: Returns False if daily budget exceeded.
  - `reset_daily_cost(self)`: Resets the daily cost tracker (called by scheduler at midnight).
  - Property `is_available -> bool`: True if at least one API key is configured and budget not exceeded.
  - Property `daily_cost -> float`: Current accumulated daily cost.

Use `instructor.from_openai(OpenAI(api_key=..., base_url=...))` for each provider. Together AI base_url: `https://api.together.ai/v1`. DeepSeek base_url: `https://api.deepseek.com`.

Install dependencies: `uv add openai instructor tenacity sentence-transformers` (do NOT add anthropic yet -- defer Claude fallback to turbocharge phase). Consolidating all Wave 1 dependency additions here so parallel plans do not race on concurrent `uv add` calls -- 04-04 must NOT run `uv add sentence-transformers`.

**test_llm_client.py** -- Tests (no live API calls):
- Test LLMClient with no API keys raises LLMUnavailableError on call()
- Test LLMClient.is_available is False when no keys configured
- Test LLMClient.is_available is False when budget exceeded
- Test _check_budget returns False after exceeding daily_budget
- Test cost tracking accumulates correctly
- Test router integration: client passes task_type through to router
- Mock the instructor client to test fallback chain behavior: first provider raises, second succeeds
- Mock all providers failing -> LLMUnavailableError raised

Use unittest.mock.patch to avoid real API calls. Test the schemas independently (Pydantic validation of valid/invalid inputs).
  </action>
  <verify>
`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/agent/test_llm_client.py -v` -- all tests pass.
`python -c "from hydra.agent.llm.client import LLMClient, LLMUnavailableError, LLMConfig; c = LLMClient(LLMConfig()); assert not c.is_available; print('disabled mode OK')"` succeeds.
  </verify>
  <done>LLM client raises LLMUnavailableError when no keys configured (AGNT-10 default). Fallback chain works through providers. Cost tracking prevents budget overruns. All tests pass without live API calls.</done>
</task>

</tasks>

<verification>
- `python -c "from hydra.agent.llm.client import LLMClient, LLMUnavailableError"` -- imports work
- `python -c "from hydra.agent.llm.schemas import DiagnosisResult, Hypothesis"` -- schemas import
- `python -m pytest tests/agent/test_llm_client.py -v` -- all tests pass
- LLMClient with empty config raises LLMUnavailableError on call() (AGNT-10 satisfied)
- LLMClient with mock API key attempts fallback chain correctly (AGNT-05 satisfied)
</verification>

<success_criteria>
LLM integration layer is built and tested. The key contract is: callers use `try: result = client.call(...) except LLMUnavailableError: use_rule_based()`. System works with zero API keys configured. Cost tracking prevents runaway spending.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-01-SUMMARY.md`
</output>
