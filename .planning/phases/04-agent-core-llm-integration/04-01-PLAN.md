---
phase: 04-agent-core-llm-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/hydra/agent/__init__.py
  - src/hydra/agent/llm/__init__.py
  - src/hydra/agent/llm/client.py
  - src/hydra/agent/llm/router.py
  - src/hydra/agent/llm/schemas.py
  - src/hydra/agent/llm/prompts.py
  - tests/test_llm_client.py
autonomous: true
requirements: [AGNT-05, AGNT-10]

user_setup:
  - service: together-ai
    why: "LLM inference for DeepSeek-R1 and Qwen models via OpenAI-compatible API"
    env_vars:
      - name: TOGETHER_API_KEY
        source: "https://api.together.ai/settings/api-keys -- create account, generate API key"
  - service: deepseek
    why: "Direct DeepSeek API for cost-optimized classification tasks"
    env_vars:
      - name: DEEPSEEK_API_KEY
        source: "https://platform.deepseek.com/api_keys -- create account, generate API key"

must_haves:
  truths:
    - "LLM client calls Together AI with structured Pydantic-validated output and receives typed responses"
    - "Fallback chain cascades through providers when primary fails (DeepSeek-R1 -> Qwen3 -> rule-based for reasoning)"
    - "Task-type router sends reasoning tasks to expensive models and classification tasks to cheap models"
    - "All LLM I/O uses Pydantic schemas -- no unvalidated JSON parsing"
    - "System operates in degraded mode with rule-based fallbacks when all LLM providers are unavailable"
    - "Daily cost tracking enforces budget cap and logs per-call costs"
  artifacts:
    - path: "src/hydra/agent/llm/client.py"
      provides: "Multi-provider LLM client with fallback chain and cost tracking"
      exports: ["LLMClient", "LLMUnavailableError"]
    - path: "src/hydra/agent/llm/router.py"
      provides: "Task-type to model routing for cost optimization"
      exports: ["TaskRouter", "TaskType"]
    - path: "src/hydra/agent/llm/schemas.py"
      provides: "All Pydantic schemas for LLM structured I/O"
      exports: ["DiagnosisResult", "Hypothesis", "DriftCategory", "MutationType", "TournamentEntry"]
    - path: "src/hydra/agent/llm/prompts.py"
      provides: "Prompt templates for each agent task type"
      exports: ["DIAGNOSIS_PROMPT", "HYPOTHESIS_PROMPT", "CLASSIFICATION_PROMPT"]
  key_links:
    - from: "src/hydra/agent/llm/client.py"
      to: "instructor library"
      via: "instructor.from_openai wrapping OpenAI client with Together AI base_url"
      pattern: "instructor\\.from_openai"
    - from: "src/hydra/agent/llm/client.py"
      to: "src/hydra/agent/llm/router.py"
      via: "TaskRouter.get_chain() returns provider/model tuples for fallback"
      pattern: "get_chain|_get_fallback_chain"
    - from: "src/hydra/agent/llm/client.py"
      to: "src/hydra/agent/llm/schemas.py"
      via: "response_model parameter accepts any Pydantic BaseModel subclass"
      pattern: "response_model.*BaseModel"
---

<objective>
Build the multi-provider LLM client with structured output, cost-optimized task routing, and fallback chains that form the foundation for all agent intelligence.

Purpose: Every LLM call in Phase 4 flows through this client. Correct fallback behavior, Pydantic validation, and cost tracking are prerequisites for the diagnostician, hypothesis engine, and all heads.

Output: `src/hydra/agent/llm/` package with client, router, schemas, and prompts. Test suite verifying fallback chain, cost tracking, and schema validation.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM schemas, prompts, router, and client package</name>
  <files>
    src/hydra/agent/__init__.py
    src/hydra/agent/llm/__init__.py
    src/hydra/agent/llm/schemas.py
    src/hydra/agent/llm/prompts.py
    src/hydra/agent/llm/router.py
    src/hydra/agent/llm/client.py
    pyproject.toml
  </files>
  <action>
    **Install dependencies** in pyproject.toml: add `openai>=1.60`, `instructor>=1.8`, `pydantic>=2.7`, `anthropic>=0.40`, `tenacity>=8.2`, `httpx>=0.27` to project dependencies. Run `uv sync`.

    **Create `src/hydra/agent/__init__.py`**: Empty package init for the agent module.

    **Create `src/hydra/agent/llm/__init__.py`**: Export LLMClient, LLMUnavailableError, TaskRouter, TaskType, and all schema classes.

    **Create `src/hydra/agent/llm/schemas.py`**: All Pydantic v2 models for LLM I/O. Follow the research code examples exactly:
    - `DriftCategory(str, Enum)`: performance_degradation, feature_distribution_drift, regime_change, overfitting, data_quality_issue
    - `DiagnosisResult(BaseModel)`: primary_cause (DriftCategory), confidence (float 0-1), evidence (list[str]), recommended_mutation_types (list[str]), reasoning (str)
    - `MutationType(str, Enum)`: hyperparameter, feature_add, feature_remove, feature_engineering, ensemble_method, prediction_target, new_data_signal
    - `Hypothesis(BaseModel)`: mutation_type (MutationType), description (str), config_diff (dict), expected_impact (str), testable_prediction (str), head_name (str)
    - `TournamentEntry(BaseModel)`: hypothesis (Hypothesis), fitness_score (float), evaluation_windows_won (int, 0-5), promoted (bool = False)
    - `DriftClassification(BaseModel)`: category (DriftCategory), severity (float 0-1), summary (str) -- for cheap classification tasks

    **Create `src/hydra/agent/llm/prompts.py`**: Prompt templates as string constants:
    - `DIAGNOSIS_PROMPT`: System prompt for root cause diagnosis from drift report. Instructs the model to analyze drift evidence, identify primary cause from the DriftCategory enum, cite specific metrics, and recommend mutation types.
    - `HYPOTHESIS_PROMPT`: System prompt for hypothesis generation from diagnosis. Instructs the model to propose mutations from the playbook matched to root cause, include specific config_diff values, and make testable predictions.
    - `CLASSIFICATION_PROMPT`: System prompt for drift categorization (lightweight, no reasoning needed). Maps drift report metrics to a DriftCategory.
    - `RESEARCH_PROMPT`: System prompt for Research Head novel signal proposals.

    **Create `src/hydra/agent/llm/router.py`**:
    - `TaskType(str, Enum)`: reasoning, classification, formatting, research, general
    - `TaskRouter` class with `get_chain(task_type: TaskType) -> list[tuple[str, str]]` that returns provider-model pairs per research:
      - reasoning: [("together", "deepseek-ai/DeepSeek-R1-0528"), ("together", "Qwen/Qwen3-235B-A22B-Instruct")]
      - classification: [("together", "Qwen/Qwen2.5-7B-Instruct-Turbo"), ("deepseek", "deepseek-chat")]
      - formatting: [("together", "Qwen/Qwen2.5-7B-Instruct-Turbo")]
      - research: [("together", "deepseek-ai/DeepSeek-R1-0528"), ("together", "Qwen/Qwen3-235B-A22B-Instruct")]
      - general: [("together", "deepseek-ai/DeepSeek-V3.1"), ("together", "Qwen/Qwen3-235B-A22B-Instruct")]
    - Include cost-per-million-tokens dict for cost tracking: `MODEL_COSTS: dict[str, tuple[float, float]]` mapping model name -> (input_cost, output_cost) per 1M tokens.

    **Create `src/hydra/agent/llm/client.py`**:
    - `LLMUnavailableError(Exception)`: Raised when all providers fail.
    - `LLMClient` class:
      - `__init__(self, config: dict)`: Takes config with `together_api_key`, `deepseek_api_key`, `daily_budget` (default 20.0). Creates instructor-wrapped OpenAI clients for Together AI (`https://api.together.ai/v1`) and DeepSeek (`https://api.deepseek.com`). Stores a `TaskRouter` instance and `_daily_cost: float = 0.0`, `_daily_budget: float`, `_call_log: list[dict]`.
      - `call(self, response_model: type[BaseModel], messages: list[dict], task_type: str = "reasoning", max_retries: int = 3) -> BaseModel`: Core method. Gets fallback chain from router. Iterates providers. For each, calls `instructor` `chat.completions.create` with `response_model` and `max_retries`. On success, tracks cost via `_track_cost()` and returns result. On failure, catches Exception and continues to next provider. If all fail, raises `LLMUnavailableError` with last error message.
      - `_track_cost(self, provider: str, model: str, messages: list[dict], result: BaseModel) -> None`: Estimates token count (rough: len(str(messages))/4 for input, len(str(result))/4 for output), looks up model cost, adds to `_daily_cost` and appends to `_call_log`.
      - `budget_remaining(self) -> float`: Returns `_daily_budget - _daily_cost`.
      - `reset_daily_budget(self) -> None`: Resets `_daily_cost` to 0 and clears `_call_log`.
      - `is_budget_exhausted(self) -> bool`: Returns `_daily_cost >= _daily_budget`.
      - Property `daily_cost -> float` and `call_log -> list[dict]`.

    Key design point per user decision: "minimize loss to LLM API costs" -- the router ensures classification tasks never hit expensive reasoning models. Budget tracking is first-class.
  </action>
  <verify>
    Run `python -c "from hydra.agent.llm import LLMClient, TaskRouter, TaskType, DiagnosisResult, Hypothesis, LLMUnavailableError; print('All imports OK')"` to verify package structure. Run `uv sync` to verify dependencies resolve.
  </verify>
  <done>
    LLM package exists with client, router, schemas, and prompts. All classes importable. Dependencies installed. Router returns correct model chains for each task type. Schemas have proper Pydantic validation (field constraints, enum values).
  </done>
</task>

<task type="auto">
  <name>Task 2: Test LLM client fallback chain and cost tracking with mocks</name>
  <files>tests/test_llm_client.py</files>
  <action>
    Create test suite that verifies LLM client behavior WITHOUT requiring real API keys (all LLM calls mocked).

    Tests:
    1. **test_router_reasoning_chain**: TaskRouter.get_chain(TaskType.REASONING) returns DeepSeek-R1 first, Qwen3 second.
    2. **test_router_classification_chain**: TaskRouter.get_chain(TaskType.CLASSIFICATION) returns Qwen2.5-7B first, DeepSeek-V3 second.
    3. **test_router_all_task_types_have_chains**: Every TaskType enum value returns a non-empty chain.
    4. **test_schema_diagnosis_validation**: DiagnosisResult rejects confidence > 1.0, missing fields, invalid DriftCategory.
    5. **test_schema_hypothesis_validation**: Hypothesis rejects invalid MutationType, validates all required fields.
    6. **test_client_fallback_on_primary_failure**: Mock Together AI to raise Exception. Verify client tries next provider in chain. (Patch instructor client create methods.)
    7. **test_client_raises_unavailable_when_all_fail**: Mock all providers to raise Exception. Verify LLMUnavailableError is raised with appropriate message.
    8. **test_cost_tracking**: Mock a successful call. Verify `daily_cost > 0`, `call_log` has one entry with provider/model/estimated_cost keys.
    9. **test_budget_exhausted**: Set daily_budget to 0.01. Mock a call that costs more. Verify `is_budget_exhausted()` returns True.
    10. **test_reset_daily_budget**: Track cost, reset, verify cost is 0 and call_log is empty.

    Use `unittest.mock.patch` to mock the instructor client's `chat.completions.create`. For schema validation tests, use `pytest.raises(ValidationError)` from pydantic.
  </action>
  <verify>Run `python -m pytest tests/test_llm_client.py -v` -- all 10 tests pass.</verify>
  <done>10 tests pass covering router chains, schema validation, fallback behavior, cost tracking, and budget management. No real API calls made.</done>
</task>

</tasks>

<verification>
1. `python -c "from hydra.agent.llm import LLMClient, TaskRouter, TaskType, DiagnosisResult, Hypothesis"` succeeds
2. `python -m pytest tests/test_llm_client.py -v` -- all tests pass
3. Router returns correct chains for all 5 task types
4. Schemas reject invalid data (confidence > 1, bad enums)
5. Client raises LLMUnavailableError when all providers fail
6. Cost tracking increments on successful calls
</verification>

<success_criteria>
LLM client package fully functional with instructor-based structured output, cost-optimized task routing across 5 task types, Pydantic schemas for all agent I/O, fallback chains that cascade correctly, and budget tracking. All 10+ tests pass with mocked LLM calls.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-01-SUMMARY.md`
</output>
