---
phase: 04-agent-core-llm-integration
plan: 03
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/hydra/agent/types.py
  - src/hydra/agent/diagnostician.py
  - src/hydra/agent/hypothesis.py
  - tests/agent/__init__.py
  - tests/agent/test_diagnostician.py
  - tests/agent/test_hypothesis.py
autonomous: true
requirements: [AGNT-02, AGNT-03]

must_haves:
  truths:
    - "Diagnostician performs structured triage from a DriftReport and produces a DiagnosisResult with primary cause, confidence, and evidence"
    - "Rule-based diagnosis maps drift signals to root causes deterministically without any LLM call"
    - "Hypothesis engine maps diagnosed root causes to candidate mutations from a curated playbook"
    - "Mutation selection uses round-robin across categories to ensure diversity"
    - "When an LLM client is available and diagnosis confidence is low, the diagnostician optionally enhances diagnosis via LLM"
  artifacts:
    - path: "src/hydra/agent/types.py"
      provides: "Shared domain types with zero LLM dependency"
      exports: ["DriftCategory", "DiagnosisResult", "MutationType", "Hypothesis"]
    - path: "src/hydra/agent/diagnostician.py"
      provides: "Structured triage from DriftReport -> DiagnosisResult"
      exports: ["Diagnostician"]
    - path: "src/hydra/agent/hypothesis.py"
      provides: "Mutation playbook and hypothesis generation"
      exports: ["HypothesisEngine", "MUTATION_PLAYBOOK"]
  key_links:
    - from: "src/hydra/agent/diagnostician.py"
      to: "src/hydra/sandbox/observer.py"
      via: "diagnostician takes DriftReport as input"
      pattern: "DriftReport"
    - from: "src/hydra/agent/hypothesis.py"
      to: "src/hydra/agent/diagnostician.py"
      via: "hypothesis engine uses DiagnosisResult.recommended_mutation_types"
      pattern: "DiagnosisResult"
---

<objective>
Build the rule-based diagnostician and mutation playbook hypothesis engine -- the deterministic core of the agent loop.

Purpose: AGNT-02 requires structured triage (data audit, SHAP attribution, regime check, overfitting test). AGNT-03 requires mutation proposals matched to root causes. Both are rule-based by DEFAULT -- this is the "Honda engine". The LLM is an optional enhancer for ambiguous cases only. The diagnostician reads a DriftReport (from Phase 3 observer) and produces a DiagnosisResult. The hypothesis engine reads a DiagnosisResult and produces Hypothesis objects from the playbook.

Output: Two modules with full test coverage and a curated mutation playbook.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
@src/hydra/sandbox/observer.py
@src/hydra/sandbox/evaluator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Shared types module, rule-based diagnostician with structured triage (TDD)</name>
  <files>
    src/hydra/agent/types.py
    src/hydra/agent/diagnostician.py
    tests/agent/__init__.py
    tests/agent/test_diagnostician.py
  </files>
  <action>
**tests/agent/__init__.py** -- Create idempotently (empty file). This plan runs in parallel with 04-01, 04-02, and 04-04. Create with `open("tests/agent/__init__.py", "a").close()` or equivalent so whichever plan runs first wins.

**types.py** -- Shared domain types with ZERO LLM dependency. Both diagnostician.py (this plan) and llm/schemas.py (04-01) import from here. This resolves the type incompatibility between 04-01's Pydantic schemas and 04-03's locally-defined dataclasses by making a single canonical source:

```python
from __future__ import annotations
from dataclasses import dataclass, field
from enum import Enum

class DriftCategory(str, Enum):
    PERFORMANCE_DEGRADATION = "performance_degradation"
    FEATURE_DISTRIBUTION_DRIFT = "feature_distribution_drift"
    REGIME_CHANGE = "regime_change"
    OVERFITTING = "overfitting"
    DATA_QUALITY_ISSUE = "data_quality_issue"

class MutationType(str, Enum):
    HYPERPARAMETER = "hyperparameter"
    FEATURE_ADD = "feature_add"
    FEATURE_REMOVE = "feature_remove"
    FEATURE_ENGINEERING = "feature_engineering"
    ENSEMBLE_METHOD = "ensemble_method"
    PREDICTION_TARGET = "prediction_target"

@dataclass
class DiagnosisResult:
    primary_cause: DriftCategory
    confidence: float  # 0.0 to 1.0
    evidence: list[str] = field(default_factory=list)
    recommended_mutation_types: list[str] = field(default_factory=list)
    reasoning: str = ""

@dataclass
class Hypothesis:
    mutation_type: MutationType
    description: str
    config_diff: dict
    expected_impact: str
    testable_prediction: str
    source: str  # "playbook" or "llm"
```

**diagnostician.py** -- Structured triage from DriftReport to DiagnosisResult:

Import from the shared types module: `from hydra.agent.types import DriftCategory, DiagnosisResult`. No Pydantic dependency in diagnostician.py -- it uses the dataclass versions from types.py.

`Diagnostician` class:
- `__init__(self, llm_client=None)`: Accept optional LLM client. If None, operates entirely rule-based.
- `diagnose(self, report: DriftReport) -> DiagnosisResult`: Main entry point.
  1. **Collect evidence** from the DriftReport -- which flags are set, which features drifted, which streaming alerts fired.
  2. **Rule-based classification** (deterministic priority order):
     - If `report.feature is not None` and `len(report.feature.drifted_features) >= 3`: primary_cause = FEATURE_DRIFT, confidence = 0.8
     - If `report.performance.sharpe_degraded` and NOT feature_drift: primary_cause = PERFORMANCE (check for overfitting via drawdown pattern), confidence = 0.7
     - If `report.performance.drawdown_alert` and streaming alerts for drawdown: primary_cause = REGIME_CHANGE, confidence = 0.6
     - If `report.performance.hit_rate_degraded` and `report.performance.calibration > 0.3`: primary_cause = OVERFITTING, confidence = 0.7
     - Default: primary_cause = PERFORMANCE, confidence = 0.5
  3. **Map cause to recommended mutation types** using a static lookup dict:
     - PERFORMANCE: ["hyperparameter", "feature_remove"]
     - FEATURE_DRIFT: ["feature_engineering", "hyperparameter"]
     - REGIME_CHANGE: ["ensemble_method", "hyperparameter"]
     - OVERFITTING: ["hyperparameter", "feature_remove"]
     - DATA_QUALITY: ["feature_remove"]
  4. **Optional LLM enhancement**: If `self.llm_client is not None` and confidence < 0.6, attempt to call `self.llm_client.call(response_model=DiagnosisResult, ...)` with the drift report as context. Wrap in try/except LLMUnavailableError to fall back to the rule-based result. The LLM call is purely additive -- it can only ENHANCE the diagnosis, never replace the rule-based path.
  5. **Build reasoning string** explaining which evidence led to the diagnosis.

**test_diagnostician.py**:
- Test pure rule-based diagnosis with sharpe_degraded=True produces PERFORMANCE cause
- Test feature drift with 3+ drifted features produces FEATURE_DRIFT cause
- Test drawdown_alert with streaming alerts produces REGIME_CHANGE cause
- Test hit_rate_degraded + high calibration produces OVERFITTING cause
- Test priority order: feature drift takes priority over performance
- Test that diagnosis works with no LLM client (None)
- Test that evidence list is populated with specific flags from the drift report
- Test recommended_mutation_types match the lookup table for each cause
- Build DriftReport test fixtures (use dataclass constructors directly from observer.py types)
  </action>
  <verify>
`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/agent/test_diagnostician.py -v` -- all tests pass.
`python -c "from hydra.agent.diagnostician import Diagnostician; print('import OK')"` succeeds.
  </verify>
  <done>Diagnostician produces structured DiagnosisResult from DriftReport using deterministic rules. Priority ordering ensures consistent diagnosis. Evidence is captured. LLM enhancement is optional. All tests pass with no LLM configured.</done>
</task>

<task type="auto">
  <name>Task 2: Mutation playbook and hypothesis engine (TDD)</name>
  <files>
    src/hydra/agent/hypothesis.py
    tests/agent/test_hypothesis.py
  </files>
  <action>
**hypothesis.py** -- Curated mutation playbook and hypothesis generation engine:

`MUTATION_PLAYBOOK: dict[str, list[dict]]` -- maps drift categories to ordered lists of candidate mutations. Use the playbook from research with these entries:
- performance_degradation: reduce_learning_rate (lr * 0.5), increase_regularization (reg_alpha * 2, reg_lambda * 2), drop_low_importance_features (bottom 3 by SHAP)
- feature_distribution_drift: shorten_training_window (window * 0.7), add_rolling_z_scores (z_score_30d, z_score_90d)
- regime_change: add_regime_conditioning (ensemble_type: regime_conditional), increase_num_leaves (num_leaves * 1.5)
- overfitting: reduce_num_leaves (max(8, num_leaves // 2)), increase_min_child_samples (min_child_samples * 2), add_dropout (feature_fraction * 0.7)
- data_quality_issue: remove_degraded_features (features_with_high_psi), reduce_training_window (window * 0.5)

Each playbook entry is a dict with keys: type (MutationType value), name (str), config_diff (dict with string expressions), rationale (str).

`HypothesisEngine` class:
- `__init__(self, playbook: dict | None = None, llm_client=None)`: Accept custom playbook override and optional LLM. Default playbook is MUTATION_PLAYBOOK.
- `_round_robin_idx: dict[str, int]`: Tracks the next mutation index per category for round-robin selection (AGNT-03 diversity requirement).
- `propose(self, diagnosis: DiagnosisResult, current_config: dict | None = None) -> Hypothesis`: Main entry point.
  1. Look up the diagnosis.primary_cause (as string value) in the playbook.
  2. Round-robin select the next mutation in that category (increment _round_robin_idx, wrap around).
  3. Build a Hypothesis object from the playbook entry.
  4. If current_config is provided, resolve config_diff expressions: "current * 0.5" -> actual value based on current_config values. Use a simple eval-free resolver: parse "current * N" and "max(N, current // M)" patterns with regex. Fall back to the raw string expression if unresolvable.
  5. Source = "playbook" for rule-based proposals.
- `propose_multiple(self, diagnosis: DiagnosisResult, n: int = 3, current_config: dict | None = None) -> list[Hypothesis]`: Return up to n hypotheses, cycling through playbook entries.
- `get_playbook_size(self, category: str) -> int`: Return the number of mutations available for a category.

**test_hypothesis.py**:
- Test propose() with PERFORMANCE diagnosis returns a valid Hypothesis from the performance playbook
- Test round-robin: calling propose() 4 times with same category cycles through entries and wraps
- Test propose_multiple() returns n hypotheses (or all available if n > playbook size)
- Test config_diff resolution: "current * 0.5" with current_config={"learning_rate": 0.1} -> 0.05
- Test config_diff fallback: unresolvable expression preserved as string
- Test unknown category returns empty list from propose_multiple (no crash)
- Test custom playbook override works
- Test all 5 playbook categories have at least 2 entries
- Test Hypothesis.source is "playbook" for rule-based proposals
  </action>
  <verify>
`cd /Users/tristanfarmer/Documents/HYDRA && python -m pytest tests/agent/test_hypothesis.py -v` -- all tests pass.
`python -c "from hydra.agent.hypothesis import HypothesisEngine, MUTATION_PLAYBOOK; assert len(MUTATION_PLAYBOOK) == 5; print('playbook OK')"` succeeds.
  </verify>
  <done>Mutation playbook covers all 5 drift categories with 2-3 mutations each. Round-robin selection ensures diversity across proposals. Config diff expressions are resolved against current model config. LLM enhancement is optional. All tests pass.</done>
</task>

</tasks>

<verification>
- `python -m pytest tests/agent/test_diagnostician.py tests/agent/test_hypothesis.py -v` -- all pass
- Diagnostician takes DriftReport, returns DiagnosisResult with cause + evidence + recommendations
- HypothesisEngine takes DiagnosisResult, returns Hypothesis from playbook with round-robin diversity
- Both work with zero LLM calls (pure rule-based)
</verification>

<success_criteria>
The deterministic diagnosis-to-hypothesis pipeline is complete. A DriftReport flows through the diagnostician to produce a diagnosis, which feeds into the hypothesis engine to produce mutation proposals from the curated playbook. This is the "Honda engine" -- it works without any LLM. Round-robin ensures experiment diversity.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-03-SUMMARY.md`
</output>
