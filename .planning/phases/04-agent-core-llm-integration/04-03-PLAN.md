---
phase: 04-agent-core-llm-integration
plan: 03
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - src/hydra/agent/diagnostician.py
  - src/hydra/agent/hypothesis.py
  - src/hydra/agent/experiment_runner.py
  - tests/test_diagnostician.py
  - tests/test_hypothesis.py
  - tests/test_experiment_runner.py
autonomous: true
requirements: [AGNT-02, AGNT-03, AGNT-04]

must_haves:
  truths:
    - "Diagnostician performs structured triage (data audit, SHAP attribution, regime check, overfitting test) from a DriftReport and produces a DiagnosisResult"
    - "Diagnostician falls back to rule-based classification when LLM is unavailable"
    - "Hypothesis engine proposes mutations from curated playbook matched to diagnosed root causes"
    - "LLM augments playbook mutations with domain-specific reasoning when available"
    - "Experiment runner executes candidate training in a subprocess with configurable timeout"
    - "Experiment runner captures stdout/stderr and returns structured result with success/failure status"
  artifacts:
    - path: "src/hydra/agent/diagnostician.py"
      provides: "Structured drift triage with LLM and rule-based fallback"
      exports: ["Diagnostician"]
    - path: "src/hydra/agent/hypothesis.py"
      provides: "Mutation playbook and hypothesis generation engine"
      exports: ["HypothesisEngine", "MUTATION_PLAYBOOK"]
    - path: "src/hydra/agent/experiment_runner.py"
      provides: "Isolated subprocess experiment execution"
      exports: ["ExperimentRunner", "ExperimentResult"]
  key_links:
    - from: "src/hydra/agent/diagnostician.py"
      to: "src/hydra/agent/llm/client.py"
      via: "LLMClient.call(response_model=DiagnosisResult) for LLM-powered diagnosis"
      pattern: "llm_client\\.call.*DiagnosisResult"
    - from: "src/hydra/agent/diagnostician.py"
      to: "src/hydra/sandbox/observer.py"
      via: "Receives DriftReport as input for triage"
      pattern: "DriftReport"
    - from: "src/hydra/agent/hypothesis.py"
      to: "src/hydra/agent/llm/schemas.py"
      via: "Produces Hypothesis objects matching schema"
      pattern: "Hypothesis|MutationType"
    - from: "src/hydra/agent/experiment_runner.py"
      to: "subprocess"
      via: "subprocess.run(timeout=N) for process isolation"
      pattern: "subprocess\\.run"
---

<objective>
Build the three core single-head agent components: diagnostician (triage drift into root causes), hypothesis engine (propose mutations from playbook), and experiment runner (execute candidate training in isolation).

Purpose: These are the workhorse modules of the agent loop. The diagnostician converts raw drift signals into actionable diagnoses. The hypothesis engine converts diagnoses into testable mutations. The experiment runner safely executes mutations in subprocess isolation. Together they form the observe-diagnose-hypothesize-experiment pipeline.

Output: Three modules in `src/hydra/agent/` with dedicated test files verifying both LLM-powered and rule-based paths.
</objective>

<execution_context>
@/Users/tristanfarmer/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tristanfarmer/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-agent-core-llm-integration/04-RESEARCH.md
@.planning/phases/04-agent-core-llm-integration/04-01-SUMMARY.md
@.planning/phases/03-sandbox-experiment-infrastructure/03-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create diagnostician and hypothesis engine with playbook</name>
  <files>
    src/hydra/agent/diagnostician.py
    src/hydra/agent/hypothesis.py
    tests/test_diagnostician.py
    tests/test_hypothesis.py
  </files>
  <action>
    **Create `src/hydra/agent/diagnostician.py`**:
    - `Diagnostician` class:
      - `__init__(self, llm_client: "LLMClient | None" = None)`: Optional LLM client; if None, always uses rule-based path (AGNT-10 degraded mode).
      - `diagnose(self, drift_report: DriftReport) -> DiagnosisResult`: Main entry point.
        1. Extract structured evidence from drift_report: performance metrics (sharpe, drawdown, hit_rate, calibration from perf report), feature drift flags (columns with PSI > threshold from feature report), streaming alerts (from streaming_alerts list).
        2. If LLM client available and not budget-exhausted:
           - Build messages with DIAGNOSIS_PROMPT as system, and drift_report evidence as user message (format the metrics, drifted features, and alerts as a structured text block).
           - Call `llm_client.call(response_model=DiagnosisResult, messages=messages, task_type="reasoning")`.
           - Return the validated DiagnosisResult.
        3. On LLMUnavailableError or if no LLM client:
           - Fall back to `_rule_based_diagnosis(drift_report)`.
      - `_rule_based_diagnosis(self, drift_report: DriftReport) -> DiagnosisResult`: Deterministic triage without LLM.
        - Check feature drift first: if any feature has PSI > 0.2, primary_cause = FEATURE_DRIFT, recommend feature_add/feature_remove.
        - Check overfitting: if train Sharpe >> OOS Sharpe (ratio > 2x from perf report if available), primary_cause = OVERFITTING, recommend reduce_num_leaves/increase_regularization.
        - Check regime change: if multiple streaming alerts AND performance degradation, primary_cause = REGIME_CHANGE, recommend ensemble_method.
        - Default: primary_cause = PERFORMANCE_DEGRADATION, recommend hyperparameter mutations.
        - confidence = 0.5 (lower than LLM-based diagnosis).
        - evidence = list of specific metrics that triggered the classification.
        - reasoning = "Rule-based classification: {logic description}".

    **Create `src/hydra/agent/hypothesis.py`**:
    - `MUTATION_PLAYBOOK: dict[str, list[dict]]`: Follow research code example exactly. Map each DriftCategory value to a list of mutation dicts with keys: type (MutationType value), name (str), config_diff (dict), rationale (str). Include all 5 categories from research (performance_degradation, feature_distribution_drift, regime_change, overfitting, data_quality_issue) with 2-4 mutations each.
    - `HypothesisEngine`:
      - `__init__(self, llm_client: "LLMClient | None" = None, playbook: dict | None = None)`: Uses provided playbook or MUTATION_PLAYBOOK default.
      - `generate(self, diagnosis: DiagnosisResult, head_name: str = "technical", max_hypotheses: int = 3) -> list[Hypothesis]`: Main entry point.
        1. Get playbook entries for `diagnosis.primary_cause.value`.
        2. If LLM client available:
           - Build messages with HYPOTHESIS_PROMPT as system, diagnosis as user context (include diagnosis.primary_cause, evidence, recommended_mutation_types).
           - Call LLM to augment/rank playbook entries. The LLM receives the playbook candidates and adds domain reasoning to select the best, customize config_diff values, and generate testable_prediction.
           - Parse response into list[Hypothesis] (up to max_hypotheses).
        3. On LLMUnavailableError or if no LLM client:
           - Fall back to `_rule_based_generate(diagnosis, head_name, max_hypotheses)`.
      - `_rule_based_generate(self, diagnosis: DiagnosisResult, head_name: str, max_hypotheses: int) -> list[Hypothesis]`: Take first N entries from playbook for the diagnosed cause. Convert each to a Hypothesis with mutation_type from entry["type"], description from entry["name"] + entry["rationale"], config_diff from entry["config_diff"], expected_impact = "Addresses {primary_cause}", testable_prediction = "Composite fitness should improve if {rationale}", head_name = head_name.

    **Create `tests/test_diagnostician.py`**:
    1. test_rule_based_diagnosis_feature_drift: DriftReport with high PSI feature -> primary_cause is FEATURE_DRIFT
    2. test_rule_based_diagnosis_performance_degradation: Default case -> PERFORMANCE_DEGRADATION
    3. test_rule_based_diagnosis_regime_change: Multiple streaming alerts + perf degradation -> REGIME_CHANGE
    4. test_llm_diagnosis_called_when_available: Mock LLM client, verify call() invoked with DiagnosisResult response_model and "reasoning" task_type
    5. test_llm_fallback_on_unavailable: Mock LLM client to raise LLMUnavailableError -> falls back to rule-based, still returns valid DiagnosisResult
    6. test_no_llm_client_uses_rules: Diagnostician(llm_client=None) always returns rule-based result

    **Create `tests/test_hypothesis.py`**:
    7. test_playbook_has_all_drift_categories: Every DriftCategory value has at least 2 playbook entries
    8. test_rule_based_generate_returns_hypotheses: Generate with no LLM -> returns list[Hypothesis] with correct fields
    9. test_rule_based_respects_max_hypotheses: max_hypotheses=1 -> returns exactly 1
    10. test_generated_hypothesis_has_valid_schema: Each returned Hypothesis passes Pydantic validation
    11. test_llm_generate_called_when_available: Mock LLM client, verify call() invoked
    12. test_llm_fallback_returns_playbook_entries: Mock LLM to fail -> returns rule-based entries

    Use mock DriftReport objects (mock the dataclass attributes: perf_report with sharpe/drawdown/hit_rate/calibration, feature_report with column drift results, streaming_alerts list, needs_diagnosis=True).
  </action>
  <verify>Run `python -m pytest tests/test_diagnostician.py tests/test_hypothesis.py -v` -- all 12 tests pass.</verify>
  <done>Diagnostician performs triage with LLM or rule-based fallback. Hypothesis engine generates from playbook with optional LLM augmentation. All 12 tests pass. Both modules gracefully degrade when LLM is unavailable (AGNT-10).</done>
</task>

<task type="auto">
  <name>Task 2: Create experiment runner with subprocess isolation</name>
  <files>
    src/hydra/agent/experiment_runner.py
    tests/test_experiment_runner.py
  </files>
  <action>
    **Create `src/hydra/agent/experiment_runner.py`**:
    - `ExperimentResult` dataclass:
      - success (bool): Whether training completed without error
      - config (dict): The configuration used
      - metrics (dict | None): Training/evaluation metrics if successful (sharpe, drawdown, etc.)
      - error (str | None): Error message if failed
      - duration_seconds (float): Wall-clock time of experiment
      - stdout (str): Captured stdout
      - stderr (str): Captured stderr
    - `ExperimentRunner`:
      - `__init__(self, timeout_seconds: int = 300, python_executable: str | None = None)`: Default 5-minute timeout. python_executable defaults to `sys.executable`.
      - `run_experiment(self, hypothesis: Hypothesis, base_config: dict) -> ExperimentResult`: Main entry point.
        1. Merge hypothesis.config_diff into base_config to create experiment_config.
        2. Serialize experiment_config to a temp JSON file.
        3. Run training script via `subprocess.run()`:
           - `[python_executable, "-m", "hydra.agent.experiment_runner", "--config", config_path]`
           - `capture_output=True, text=True, timeout=timeout_seconds`
        4. Parse stdout for JSON result line (last line starting with `{"metrics":...}`)
        5. Return ExperimentResult with parsed metrics on success.
        6. On `subprocess.TimeoutExpired`: return ExperimentResult(success=False, error="Timeout after {N}s")
        7. On non-zero returncode: return ExperimentResult(success=False, error=stderr)
      - `_merge_config(self, base: dict, diff: dict) -> dict`: Deep merge config_diff into base_config. For string values containing "current", evaluate relative to base (e.g., "current * 0.5" applied to base value). For other values, direct replacement.

    Add a `if __name__ == "__main__":` block at the bottom of experiment_runner.py that:
    - Parses --config argument
    - Loads config JSON
    - Placeholder: prints `{"metrics": {"sharpe": 0.0, "drawdown": 0.0}, "status": "placeholder"}` -- actual training integration is done by the agent loop when it wires to BaselineModel.
    - This block exists so subprocess.run can invoke the module.

    **Create `tests/test_experiment_runner.py`**:
    1. test_run_experiment_success: Create a temp Python script that prints `{"metrics": {"sharpe": 1.5}}` and exits 0. Run through ExperimentRunner. Verify ExperimentResult.success=True, metrics["sharpe"]=1.5.
    2. test_run_experiment_timeout: Create a temp script that `time.sleep(10)`. Run with timeout_seconds=1. Verify ExperimentResult.success=False, "Timeout" in error.
    3. test_run_experiment_error: Create a temp script that raises ValueError. Verify ExperimentResult.success=False, error contains traceback info.
    4. test_merge_config_direct: {"lr": 0.05} merged into {"lr": 0.1} -> {"lr": 0.05}
    5. test_merge_config_preserves_unmodified: Diff has one key, rest of base preserved.
    6. test_experiment_result_captures_duration: Duration > 0 for any run.

    For tests 1-3, create actual temp .py script files (using tempfile.NamedTemporaryFile) rather than invoking the module itself. Override the subprocess command in ExperimentRunner to point at the temp scripts.
  </action>
  <verify>Run `python -m pytest tests/test_experiment_runner.py -v` -- all 6 tests pass.</verify>
  <done>Experiment runner executes training in isolated subprocess with timeout. Returns structured ExperimentResult with metrics, error, stdout, stderr, and duration. 6 tests pass covering success, timeout, error, and config merge cases.</done>
</task>

</tasks>

<verification>
1. `python -c "from hydra.agent.diagnostician import Diagnostician; from hydra.agent.hypothesis import HypothesisEngine, MUTATION_PLAYBOOK; from hydra.agent.experiment_runner import ExperimentRunner"` succeeds
2. `python -m pytest tests/test_diagnostician.py tests/test_hypothesis.py tests/test_experiment_runner.py -v` -- all 18 tests pass
3. Diagnostician produces valid DiagnosisResult for both LLM and rule-based paths
4. Hypothesis engine returns valid Hypothesis objects from playbook
5. Experiment runner respects timeout and captures subprocess output
</verification>

<success_criteria>
Three core agent components operational: diagnostician performs structured triage (4 check types) with LLM or rule-based fallback, hypothesis engine proposes mutations from curated playbook (5 categories, 2-4 mutations each) with optional LLM augmentation, and experiment runner executes candidates in subprocess isolation with configurable timeout. All 18 tests pass.
</success_criteria>

<output>
After completion, create `.planning/phases/04-agent-core-llm-integration/04-03-SUMMARY.md`
</output>
